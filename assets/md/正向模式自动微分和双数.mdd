![正向模式自动微分和双数](https://towardsdatascience.com/forward-mode-automatic-differentiation-dual-numbers-8f47351064bf)
正向模式自动微分和双数
===========

[![罗伯特兰格](https://miro.medium.com/fit/c/48/48/2*SW5oO5c9wOWLOSL3TosYCA.jpeg)](/@RobertTLange?source=post_page-----8f47351064bf----------------------)

[罗伯特兰格](/@RobertTLange?source=post_page-----8f47351064bf----------------------)

跟随

[9月2日](/forward-mode-automatic-differentiation-dual-numbers-8f47351064bf?source=post_page-----8f47351064bf----------------------) · 11 分钟阅读

自动分化（AD）是深度学习成功故事背后的驱动力之一。它允许我们有效地计算我们最喜欢的组合函数的梯度评估。TensorFlow，PyTorch和所有前辈都使用AD。沿着随机逼近技术，如SGD（及其所有变体），这些梯度改进了我们最喜欢的网络架构的参数。

许多人（直到最近包括我自己）都认为反向传播，连锁规则和自动区分是同义词。但他们不是。为了成为真正的工艺大师，我们必须找出原因。🔥

**TL; DR**：_我们讨论区分计算机程序的不同方法。更具体地说，我们比较正向模式和反向模式（backprop）自动微分。最终，我们使用双重数字实现正向模式AD，以实现简单的逻辑回归问题。可以在_[_这里_](https://github.com/RobertTLange/code-and-blog/blob/master/01_dual_number_ad/dual_number_ad.ipynb)_找到简单的numpy代码__。_

计算计算机程序的衍生物
===========

“可区分编程”这个术语可能有点令人难以置信。通常，程序与分析或可区分函数无关。除了评估数学函数的计算机功能。想象一下，您面临着适合某些逻辑回归模型的任务。一种方法是通过迭代最小化逻辑输出函数的二进制交叉熵损失给定一些真正的标记数据：

![](https://miro.medium.com/max/30/1*oWA5LIgpHjN80-YhREnu4Q.png?q=20)

![](https://miro.medium.com/max/835/1*oWA5LIgpHjN80-YhREnu4Q.png)

分析表达式可以很容易地转换为一些Python代码：

该函数基于固定的真实标签（`y_true`）和预测/拟合标签概率（`y_pred`）计算数值。在我们的情况下，`y_pred`相当于S形压扁输出，这又是β的函数。为了找到最佳拟合系数，我们需要遵循最陡的下降方向（也就是梯度）来最小化损失函数。为此，我们可以访问一组差异化技术：

1.  **手动区分**（🙌）：正如人们已经知道的那样，手动区分需要一个人在一张纸上计算出衍生物。之后，该派生可以在代码中实现它。这样做很容易出现“肮脏”错误，并且（至少对我来说）相当耗时。祝你好运，想要深入挖掘网络。
2.  **数值微分**（💻）：可以简单地尝试使用两个无穷小的不同输入来近似梯度，而不是用链和商数规则来弄脏手。通过这样做，我们近似了导数的形式定义。这有两个主要缺点：它需要多个函数评估（每个输入维度至少两次）并且受到舍入错误以及截断错误（输入从不完全相同）的影响。从好的方面来说 - 它非常容易实现。
3.  **符号微分**（🈂）：符号方法应用符合微分规则的变换，以获得导数的表达式。这是在Mathematica等软件中实现的，并依赖于一些严肃的代数操作。组合函数存在导致衍生符号过于复杂的风险。这通常被称为“表达肿胀”。最终，这可能导致梯度的任意昂贵的评估。
4.  **自动微分**（🔁）：AD可以在每个可能的时间点简化衍生表达，而不是膨胀到无穷大。例如每次操作后！所有数学都可以看作是一组有限的基本运算（加法，乘法，取幂等）的组合。对于这样的操作，我们已经知道了衍生物的功能形式。通过链规则的优点，我们可以结合这些基本导数，并以内存存储为代价降低表达式的复杂性。

![](https://miro.medium.com/max/30/0*XAeFQ_5Qrha5CNoL.jpg?q=20)

![](https://miro.medium.com/max/1600/0*XAeFQ_5Qrha5CNoL.jpg)

AD的核心融合了数字和符号世界的精华。它借助于符号微分的基本规则集来计算导数的值。为了克服肿胀，在每个阶段都简化了符号表达。只需通过数值评估先前计算的结果（或简单地输入数据）。因此，它没有为衍生物本身提供解析表达式。相反，它迭代地评估给定数据的梯度。简单地说：f'（2）≠f'（x）。

让我们看看这如何应用于我们的逻辑回归问题并将逐步输出定义为（h）=σ（h）和h（X，β）=Xβ。然后我们可以将感兴趣的梯度重写为：

![](https://miro.medium.com/max/30/1*0X453WMP7PMNVCaDHxiOBA.png?q=20)

![](https://miro.medium.com/max/1143/1*0X453WMP7PMNVCaDHxiOBA.png)

上面的表达式是手动推导。乍一看，这可能看起来不像是简化。但是我们现在可以观察到更一般的原理：梯度可以分解为单个转换路径的梯度组件：

1.  预激活h（X，β）及其梯度∇h（X，β）与参数β一致。
2.  激活a（h）及其梯度∇a（h）与预激活h。
3.  类probs的日志记录a（h）和梯度对角矩阵∇loga（h）和激活a。

对于每个部分评估h（X，β），a（h），log a（h），我们可以容易地计算对它们各自的输入β，h，a的灵敏度并获得它们的导数。最终，真实概率向量p提供缩放梯度的误差信号。

AD：前进与后退模式
==========

我们可以通过两种不同的方式_自动_计算渐变的各个组件：

1.  **前向累积模式**（⏩）：简单地提出模式将Leibniz的链规则应用于_前向原始轨迹_中的每个基本操作。从而，获得_导数迹线_。在每个阶段，我们都会在“锁步”中评估运算符及其梯度！

*   原始痕迹：h（X，β）→a（h）→log a（h）
*   导数轨迹：∇h（X，β）→∇a（h）→∇loga（h）
*   锁步评估+衍生：

![](https://miro.medium.com/max/30/1*8u2rCR3kraMkhwsvwBk3sQ.png?q=20)

![](https://miro.medium.com/max/921/1*8u2rCR3kraMkhwsvwBk3sQ.png)

最后，整体梯度∇ _大号_然后由单独的块相乘并用对重新缩放获得。

1.  **反向累积模式**（⏪）：另一方面，反向模式不会同时计算衍生物，但需要两个单独的阶段。在_前进阶段_期间，评估所有中间变量并将它们的值存储在存储器中。在随后的_后退阶段，_我们再次借助于链规则传播衍生物/邻接。反向模式AD是我们在深度学习中通常称为**反向传播**（Rumelhart et al。，1988）。

*   前进阶段：h（X，β）→a（h）→log a（h）
*   后退阶段：∇日a（h）→∇a（h）→∇h（X，β）。那么我们如何知道以何种方式发送渐变？我们需要一个簿记方案，该方案由预先指定的_计算图_提供，该_图_指定了操作流程。

比较操作模式时有两个主要考虑因素：

1.  **存储器存储与计算时间**：正向模式要求我们存储衍生物，而反向模式AD仅需要存储激活。前向模式AD在变量评估的同时计算导数，而backprop在单独的后向阶段中进行计算。
2.  **输入与输出维数**：给定函数f，我们可以根据要处理的维度区分两种方案：

*   n << m：输入尺寸小于输出。应用的操作从一组几个单元扩展维度。→正向模式在计算上比反向模式便宜。
*   n >> m：输入尺寸大于输出。这是经典的深度学习设置。我们从前向传递获得标量损失评估并传播回相当高维度的输入。→反向模式在计算上比正向模式便宜。

![](https://miro.medium.com/max/30/0*kgBP3w8p4u0FXicq.jpg?q=20)

![](https://miro.medium.com/max/1600/0*kgBP3w8p4u0FXicq.jpg)

我们有兴趣将计算从几个单元推向许多单元。事情并不一定是明确的，因为人们可以相当容易地向前模式计算。因此，不需要单独的前向通行证。我假设反向模式在DL中的主导地位主要来自降低的内存需求。

双重数字的作用
=======

因此，这里出现了一种简化前向模式AD的代数魔术技巧：可以证明前向模式AD相当于用双数来评估感兴趣的函数。

![](https://miro.medium.com/max/30/0*9uNmDCU8Xkijyv7Z.jpg?q=20)

![](https://miro.medium.com/max/1600/0*9uNmDCU8Xkijyv7Z.jpg)

我对这篇博文的动机源于对我对这种等价性的理解不满意。所以我不得不深入挖掘一下。现在让我尝试对使用任何数值评估的双重部分来计算其各自参数梯度的优雅进行阐述。那么双重数字又是什么呢？我们可以将输入变量x分解为实数和双重部分：

![](https://miro.medium.com/max/30/1*8cLobWj5ov5jCXzAXqnYjg.png?q=20)

v，v̇实数，ε≠0和ε²= 0.根据这个简单的定义，得到以下基本算术属性：

![](https://miro.medium.com/max/30/1*pVna1IGn22WzBgPoUS6gxw.png?q=20)

![](https://miro.medium.com/max/620/1*pVna1IGn22WzBgPoUS6gxw.png)

此外，人们可以代数推导出以下内容：

![](https://miro.medium.com/max/30/1*dmdmalZy-HY8dyXDQxJsnA.png?q=20)

![](https://miro.medium.com/max/531/1*dmdmalZy-HY8dyXDQxJsnA.png)

哇！通过以其双重形式评估f（x）并设定v̇= 1，我们能够以ε前面的系数的形式恢复函数值f（v）以及其评估的导数f'（v）。最后，链规则平滑地转换为双重设置：

![](https://miro.medium.com/max/30/1*mox0oJuFjDLvnJ2r55e3QQ.png?q=20)

![](https://miro.medium.com/max/628/1*mox0oJuFjDLvnJ2r55e3QQ.png)

这意味着我们可以轻松地在计算层之间传播渐变，只需将衍生物相互相乘即可。为了实现双数，我们只需要一个单独的存储系统来跟踪x =（v，前面的系数v̇）并将相应的导数计算应用于x的双重部分。很棒的代数！

对于多变量函数，事情变得有点棘手。想象一下两个输入：

![](https://miro.medium.com/max/30/1*Y4UUm155e81yJ2snGHT3WA.png?q=20)

![](https://miro.medium.com/max/157/1*Y4UUm155e81yJ2snGHT3WA.png)

通过设置v = 1和u = 0来计算关于x的偏导数。对于y，我们必须翻转两个位。为了并行执行此操作并防止多个前向传递，我们可以对事物进行矢量化并保持对角矩阵：

![](https://miro.medium.com/max/30/1*Y5miIKk6tDObGCP2EgMlBQ.png?q=20)

![](https://miro.medium.com/max/146/1*Y5miIKk6tDObGCP2EgMlBQ.png)

每行代表一个偏导数。所有上述规则然后很容易转化为多变量情况。

所以仍然需要讨论如何思考ε？我从这篇[博文中](https://blog.demofox.org/2014/12/30/dual-numbers-automatic-differentiation/)偷走的两个直觉如下：

1.  ε可以被认为是无穷小数的一种形式。当对这么小的数字求平方时，它就变得不可表示了。直观地，这类似于数值微分步长。
2.  考虑ε的另一种可能方式是作为矩阵：

![](https://miro.medium.com/max/30/1*f_qwDPaHYb8DwuKVzk5x1g.png?q=20)

![](https://miro.medium.com/max/146/1*f_qwDPaHYb8DwuKVzk5x1g.png)

计算εxε然后产生所需的空矩阵。通过限制i²= -1个虚数，帮助我们计算旋转。另一方面，双数字限制ε²= 0，并允许有效和精确的导数评估。让我们看看我们如何将其转换为逻辑回归示例的代码。

让我们编码吧！
=======

首先，我们需要一些合成数据来解决二进制分类问题。下面的类对象生成系数，高斯噪声和特征。然后，我们采用点积和阈值转换后的特征，以获得二进制标签。此外，还有一些小工具可以对批量数据进行采样并在时期之间对数据进行混洗。所有非常标准的东西。什么都没有。

让我们生成一些数据！

接下来让我们来定义事物的“双重肉”。我们需要确保所有基本操作（加法，乘法，sigmoid激活以及渐变所需的对数）作用于矢量的双重表示的实部和双部分（需要渐变） ）。因此，我们需要一个双包装器来重载/重新定义运算符。

让我们设置一些双数字操作

sigmoid函数的双版本将标准$ \\ sigma（。）$应用于实部，并使用其评估的导数更改维度$ n\_ {batch} \\ times d $的双矩阵的数值。由于$ \\ sigma'（h）= \\ sigma（h）（1 - \\ sigma（h））$被应用于向量$ h $的每个元素。因此，双重部分的整体变化由下式给出：

这又产生了一个大小为$ n\_ {batch} \\ times d $的雅可比行列式。相同的概念转移到日志操作员。现在我们为双数运算重新定义了算术设置，让我们研究一下如何获得逻辑回归预测和相应的二元交叉熵损失：

简单的“前向”传递和双重损失函数评估

最后，让我们将所有内容放在最后的训练循环中。我们生成二进制数据并初始化一些占位符列表以存储我们的中间结果。然后，我们计算Sklearn解算系数来衡量我们离开的距离。然后我们可以简单地循环遍历各个批次。我们将系数初始化为双张量，样本批次，将梯度系数/对角矩阵设置回其偏导数初始化。我们获得预测和损失。双重损失评估的对应感兴趣∇梯度_大号_。然后，SGD更新只需采用双重操作并在最陡的下降方向上执行一步。

让我们训练一个简单的例子，看看指标：

运行训练循环！

这个理论很有意思。损失随着处理的批次数量而减少。系数收敛于Sklearn解决方案，训练精确地发射天空火箭。来自A队的汉尼拔会说“我喜欢它，当计划在一起时”。

![](https://miro.medium.com/max/30/0*s7q9wheeh88EvGGn.jpg?q=20)

![](https://miro.medium.com/max/1600/0*s7q9wheeh88EvGGn.jpg)

有效地计算Hessian-Vector产品
=====================

我们已经讨论了为什么反向传播和反向模式AD对于标准深度学习应用程序更有效。前向模式AD可能仍然派上用场的一个原因是它在计算Hessian矢量产品Hv时的实用性。我们可以使用**反向前进**配置来组合正向和反向模式来计算二阶Hessian。更具体地说，给定带有输入x的函数f，我们可以简单地同时使用两种模式：

1.  正向模式：计算梯度向量积∇ _FV_通过设定X = V。
2.  反向模式：获取结果并对其应用反向传播：∇²fv= Hv

聪明，对吗？然后可以使用Hessian进行高阶优化，该优化也考虑损耗表面的近似曲率。总结：对于前进模式AD仍有一些爱。

我们已经看到了前向模式自动区分的强大功能和局限性。它允许我们同时评估函数及其派生。因此，人们能够克服反向模式AD又称backprop的两阶段原则。这是以存储所有中间节点的双重表示为代价的。此外，单独的后向传递的计算工作量转移到前向传递。我们看到了双数的神奇代数性质如何让我们有效而准确地完成这一过程。

结论
==

总而言之，我发现它非常令人满意，并且从头开始实施事物具有超凡的洞察力。我认为当前的DL库及其挑战（例如静态与动态计算图，存储与计算）有很大不同。我建议感兴趣的读者仔细看看Baydin等人的一项伟大调查。（2018）。这篇论文是我的出发点，提供了一个非常易读的概述！您也可以在[这里](https://github.com/RobertTLange/code-and-blog)找到所有代码。

1.  Baydin，AG，BA Pearlmutter，AA Radul和JM Siskind。（2018）：“机器学习中的自动差异化：一项调查” _，机器学习研究期刊_，18。
2.  Rumelhart，DE，GE Hinton，RJ Williams等。（1988）：“通过反向传播错误学习表示”，_认知建模_，5,1。