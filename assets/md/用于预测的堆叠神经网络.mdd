![用于预测的堆叠神经网络](https://towardsdatascience.com/stacked-neural-networks-for-prediction-415ef3b04826)
用于预测的堆叠神经网络
===========

[

![Vivek Palaniappan](https://miro.medium.com/fit/c/48/48/1*3XLAHFeP1iEJQF57Q7ipRg.jpeg)

](/@VivekPa69?source=post_page-----415ef3b04826----------------------)

[Vivek Palaniappan](/@VivekPa69?source=post_page-----415ef3b04826----------------------)

跟随

[2018年](/stacked-neural-networks-for-prediction-415ef3b04826?source=post_page-----415ef3b04826----------------------) 10 [月17日](/stacked-neural-networks-for-prediction-415ef3b04826?source=post_page-----415ef3b04826----------------------) · 10 分钟阅读

![](https://miro.medium.com/max/30/1*jvdJ19DifEAJ06VbXQ75IA.jpeg?q=20)

![](https://miro.medium.com/max/2600/1*jvdJ19DifEAJ06VbXQ75IA.jpeg)

机器学习和深度学习已经在金融机构中找到了自己的位置，因为它们可以高精度地预测时间序列数据。有很多研究正在进行改进模型，以便他们能够更准确地预测数据。这篇文章是关于我的项目[AIAlpha的一篇文章](https://github.com/VivekPa/AIAlpha)，这是一个**堆叠的神经网络架构**，可以预测各个公司的股票价格。该项目也是iNTUtion 2018 的**入围者**之一，这是新加坡本科生的黑客马拉松。

工作流程
----

该项目的工作流程基本上是以下步骤：

1.  获取股票价格数据
2.  使用**小波变换去噪**数据
3.  使用**Stacked Autoencoders**提取功能
4.  使用功能训练**LSTM**
5.  预测准确性的测试模型

在这篇文章中，我将详细介绍每个步骤以及为什么我选择做出某些决定。

数据采集
----

`pandas_datareader`借助雅虎财经的API，很容易获得股票价格数据。因此，它只是简单地完成`stock_data = pdr.get_data_yahoo(self.ticker, self.start, self.end)`。

**去噪数据**
--------

由于股票市场动态的复杂性，股票价格数据往往充满噪音，可能分散机器学习从学习趋势和结构。因此，删除一些噪声符合我们的利益，同时保留数据中的趋势和结构。起初，我想使用傅里叶变换（那些不熟悉的人应该[阅读这篇文章](https://medium.com/engineer-quant/fourier-analysis-ii-724c20ee27c1)），但我认为小波变换可能是保留数据时间因素的更好选择，而不是仅产生基于频率的输出。

**小波变换**
--------

小波变换与傅立叶变换密切相关，只是用于变换的函数是不同的，并且这种变换发生的方式也略有不同。

![](https://miro.medium.com/max/30/1*BWC7NnXrYPtE9tGaga8VOg.png?q=20)

![](https://miro.medium.com/max/330/1*BWC7NnXrYPtE9tGaga8VOg.png)

过程如下：使用小波变换对数据进行变换，然后去除超过完全标准偏差的系数（在所有系数中），并对新系数进行逆变换以获得去噪数据。

以下是小波变换如何对时间序列数据进行去噪的示例：

![](https://miro.medium.com/freeze/max/30/1*F9nx1dKkezGky8n-v5asQw.gif?q=20)

![](https://miro.medium.com/max/469/1*F9nx1dKkezGky8n-v5asQw.gif)

如您所见，在去噪版本中不存在初始信号中存在的随机噪声。这正是我们要查看的股票价格数据。

以下是如何对数据进行去噪的代码：

x = np.array（self.stock\_data.iloc \[i：i + 11，j\]）   
（ca，cd）= pywt.dwt（x，“haar”）   
cat = pywt.threshold（ca，np.std（ca） ），mode =“soft”）   
cdt = pywt.threshold（cd，np.std（cd），mode =“soft”）   
tx = pywt.idwt（cat，cdt，“haar”）

该库`pywt`非常适合小波变换，极大地减轻了我的负担。

提取功能
----

在通常的机器学习环境中，提取特征将需要专家领域知识。这是我没有的奢侈品。我或许可以尝试使用某种形式的技术指标，如移动平均线或移动平均收敛差异（MACD）或动量指标，但我觉得盲目地使用它可能不是最优的。

但是，可以通过使用堆叠自动编码器或其他机器学习算法（如受限制的Boltzmann机器）来实现自动特征提取。由于编码的可解释性与受限制的Boltzmann机器的概率相比，我选择使用堆叠自动编码器。

堆叠式自动编码器
--------

从本质上讲，堆叠式自动编码器非常适合压缩数据并再次重放数据。我们感兴趣的是压缩部分，因为它意味着重现数据所需的信息都以某种方式以压缩形式编码。这表明这些压缩数据在某种程度上可能是我们试图从中提取特征的数据的特征。以下是堆叠自动编码器的网络结构：

![](https://miro.medium.com/max/30/1*Xwdj4deylOWIOjv9YH1XNw.png?q=20)

![](https://miro.medium.com/max/827/1*Xwdj4deylOWIOjv9YH1XNw.png)

输入数据被压缩成所需的许多神经元，并且网络被迫使用自动编码器重建初始数据。这会强制模型提取数据的关键元素，我们可以将其解释为要素。需要注意的一点是，由于没有输入输出对，这个模型实际上属于无监督学习，但输入和输出都是相同的。

我们可以使用`keras`构建这样的模型，使用功能API而不是顺序API更有用。

class AutoEncoder：  
 def \_\_init \_\_（self，encoding\_dim）：  
 self.encoding\_dim = encoding\_dim   
  
 def build\_train\_model（self，input\_shape，encoded1\_shape，encoded2\_shape，decoding1\_shape，decoding2\_shape）：  
 input\_data = Input（shape =（1，input\_shape））  
  
 encoded1 = Dense（encoded1\_shape， activation =“relu”，activity\_regularizer = regularizers.l2（0））（input\_data）  
 encoded2 = Dense（encoded2\_shape，activation =“relu”，activity\_regularizer = regularizers.l2（0））（encoded1）  
 encoded3 = Dense（self.encoding\_dim， activation =“relu”，activity\_regularizer = regularizers.l2（0））（encoded2）  
 decoding1 = Dense（decoding1\_shape，activation =“relu”，activity\_regularizer = regularizers.l2（0））（encoded3）  
 decoding2 =密集（decoding2\_shape，activation =“relu”，activity\_regularizer = regularizers.l2（0））（decoding1）decoding   
 =密集（input\_shape，activation =“sigmoid”，activity\_regularizer = regularizers.l2（0））（decoding2）  
  
 autoencoder =模型（输入= input\_data，输出=已解码）  
  
 编码器=模型（input\_data，encoded3）  
  
 ＃现在使用我们已预处理的数据训练模型  
 autoencoder.compile（loss =“mean\_squared\_error”，optimizer =“adam”）  
  
 train = pd.read\_csv（ “preprocessing / rbm\_train.csv”，index\_col = 0）  
 ntrain = np.array（train）  
 train\_data = np.reshape（ntrain，（len（ntrain），1，input\_shape））  
  
 #print（train\_data）  
 #autoencoder。摘要（）  
 autoencoder.fit（train\_data，train\_data，epochs = 1000）

我使用2000年至2008年的去噪股票价格数据训练了自动编码器。经过1000个时期的训练后，RMSE降至0.9左右。然后，我使用该模型将剩余的股票价格数据编码为特征。

LSTM模型
------

LSTM模型不需要介绍，因为它在预测时间序列中变得非常普遍和流行。它从细胞状态的存在中获得其卓越的预测能力，使其能够理解和学习数据中的长期趋势。这对我们的股票价格数据尤为重要。我将在下面讨论我认为重要的设计选择的某些方面。

优化
--

所使用的优化器类型可以极大地影响算法收敛到最小值的速度。此外，重要的是存在一些随机性概念，以避免陷入局部最小值并且不会达到全局最小值。有一些很棒的算法，但我选择使用Adam优化器。Adam优化器结合了另外两个优化器的特权：ADAgrad和RMSprop。

ADAgrad优化器基本上对每个参数和每个时间步使用不同的学习速率。ADAgrad背后的原因是，不频繁的参数必须具有较大的学习率，而频繁的参数必须具有较小的学习率。换句话说，ADAgrad的随机梯度下降更新成为

![](https://miro.medium.com/max/30/1*kvAqZRKPz9ejUFpZYYHSKw.png?q=20)

![](https://miro.medium.com/max/166/1*kvAqZRKPz9ejUFpZYYHSKw.png)

哪里

![](https://miro.medium.com/max/30/1*YHmO1XvtdxzYOTn132k8sw.png?q=20)

![](https://miro.medium.com/max/144/1*YHmO1XvtdxzYOTn132k8sw.png)

基于已经为每个参数计算的过去梯度来计算学习速率。因此，

![](https://miro.medium.com/max/30/1*scknMeodvsTWu51mAsVKfQ.png?q=20)

![](https://miro.medium.com/max/219/1*scknMeodvsTWu51mAsVKfQ.png)

其中G是过去梯度的平方和的矩阵。这种优化的问题在于随着迭代次数的增加，学习率开始迅速消失。

RMSprop考虑仅通过使用一定数量的先前梯度来修复递减的学习速率。更新成为

![](https://miro.medium.com/max/30/1*ThFzFbGn_7eLc4NsYJnsng.png?q=20)

![](https://miro.medium.com/max/254/1*ThFzFbGn_7eLc4NsYJnsng.png)

哪里

![](https://miro.medium.com/max/30/1*W3SgYBYoZTUdmDI9vYYrBA.png?q=20)

![](https://miro.medium.com/max/257/1*W3SgYBYoZTUdmDI9vYYrBA.png)

现在我们已经了解了这两个优化器的工作原理，我们可以了解一下Adam的工作原理。

自适应矩估计（Adam）是另一种通过考虑过去平方梯度的指数衰减平均值和过去梯度的指数衰减平均值来计算每个参数的自适应学习率的方法。这可以表示为

![](https://miro.medium.com/max/30/1*1XBLYxpfxwV3lJ474HJKsg.png?q=20)

![](https://miro.medium.com/max/205/1*1XBLYxpfxwV3lJ474HJKsg.png)

![](https://miro.medium.com/max/30/1*lJROSFa5wCQxEe3hnD0WcA.png?q=20)

![](https://miro.medium.com/max/219/1*lJROSFa5wCQxEe3hnD0WcA.png)

v和m可以分别被认为是梯度的第一和第二矩的估计，因此得到自适应矩估计的名称。当这是第一次使用时，研究人员观察到存在0的固有偏差，他们通过使用以下估计来反驳这一点：

![](https://miro.medium.com/max/30/1*oOMVcAZbaNCPz4rRVPaHNQ.png?q=20)

![](https://miro.medium.com/max/104/1*oOMVcAZbaNCPz4rRVPaHNQ.png)

![](https://miro.medium.com/max/30/1*W5IaGYMPmFC_r-rMmNwvwg.png?q=20)

![](https://miro.medium.com/max/113/1*W5IaGYMPmFC_r-rMmNwvwg.png)

这导致我们进入最终的梯度更新规则

![](https://miro.medium.com/max/30/1*FWL7FVvEh6S1xqEJs4bfKg.png?q=20)

![](https://miro.medium.com/max/220/1*FWL7FVvEh6S1xqEJs4bfKg.png)

这是我使用的优化器，其好处总结如下：

1.  每个参数和每次迭代的学习速率都不同。
2.  与ADAgrad一样，学习并没有减少。
3.  渐变更新使用权重分布的时刻，允许更统计的声音下降。

正则
--

训练模型的另一个重要方面是确保权重不会太大并开始关注一个数据点，因此过度拟合。所以我们应该总是包括对大权重的惩罚（大的定义将取决于所使用的常规者的类型）。我选择使用Tikhonov正则化，可以将其视为以下最小化问题：

![](https://miro.medium.com/max/30/1*z0jHvxgHm1mvG24FlVgzAQ.png?q=20)

![](https://miro.medium.com/max/315/1*z0jHvxgHm1mvG24FlVgzAQ.png)

函数空间在再生核Hilbert空间（RKHS）中的事实确保了规范的概念存在。这允许我们将规范的概念编码到我们的常规器中。

辍学
--

一种更新的防止过度拟合的方法考虑当一些神经元突然不起作用时会发生什么。这迫使模型不要过度依赖任何神经元组，并考虑所有这些神经元。辍学已经发现它们可以使神经元更加健壮，从而使它们能够预测趋势，而无需关注任何一个神经元。以下是使用辍学的结果

![](https://miro.medium.com/max/30/1*FESd4n1sGXTSN5cvpEy8qQ.png?q=20)

![](https://miro.medium.com/max/747/1*FESd4n1sGXTSN5cvpEy8qQ.png)

正如您所知，当出现辍学时，错误会继续减少而不会导致错误退出。

模型实施
----

由于`keras`及其功能API，上述所有分析都可以相对轻松地实现。这是模型的代码（查看整个代码，查看我的GitHub：[AlphaAI](https://github.com/VivekPa/AlphaAI)）

class NeuralNetwork：  
 def \_\_init \_\_（self，input\_shape，stock\_or\_return）：  
 self.input\_shape = input\_shape   
 self.stock\_or\_return = stock\_or\_return   
  
 def make\_train\_model（self）：  
 input\_data = kl.Input（shape =（1，self.input\_shape））  
 lstm = kl.LSTM （5，input\_shape =（1，self.input\_shape），return\_sequences = True，activity\_regularizer = regularizers.l2（0.003），  
 recurrent\_regularizer = regularizers.l2（0），dropout = 0.2，recurrent\_dropout = 0.2）（input\_data）  
 perc = kl。密集（5，激活=“sigmoid”，activity\_regularizer =正规化器.l2（0.005））（lstm）  
 lstm2 = kl.LSTM（2，activity\_regularizer = regularizers.l2（0.01），recurrent\_regularizer = regularizers.l2（0.001），  
  
 dropout                        = 0.2，recurrent\_dropout = 0.2）（perc）out = kl.Dense（1，activation =“sigmoid”，activity\_regularizer = regularizers.l2（0.001））（lstm2）  
  
 model = Model（input\_data，out）  
 model.compile（optimizer） =“adam”，loss =“mean\_squared\_error”，metrics = \[“mse”\]）  
  
 #load data   
  
 train = np.reshape（np.array（pd.read\_csv（“features / autoencoded\_train\_data.csv”，index\_col = 0）），  
 （len（np.array（pd.read\_csv（“features / autoencoded\_train\_data.csv”））），1，self.input\_shape））  
 train\_y = np.array（pd.read\_csv（“features / autoencoded\_train\_y.csv”，index\_col = 0 ））  
 ＃train\_stock = np.array（pd.read\_csv（“train\_stock.csv”）））   
  
 ＃train model  
  
 model.fit（train，train\_y，epochs = 2000）

结果
--

这些是我对各公司的预测结果。

![](https://miro.medium.com/max/30/1*-sMYhYqlv91XNJYI1wXgSw.png?q=20)

![](https://miro.medium.com/max/640/1*-sMYhYqlv91XNJYI1wXgSw.png)

![](https://miro.medium.com/max/30/1*xT7l7vdMkn6t3l976TNzaw.png?q=20)

![](https://miro.medium.com/max/640/1*xT7l7vdMkn6t3l976TNzaw.png)

很明显，使用这种神经网络架构的结果是不错的，并且如果实施到策略中可以是有利可图的。

在线学习
----

除了从历史数据中学习模型之外，我还想让模型始终学习，甚至从预测中学习。因此，我已经使它成为一个学习和预测的在线模型。换句话说，它学习历史数据，预测明天的价格，明天，当实际价格已知时，它也会学习使用它。所以模型总是在改进。

除了使用实际价格来改善之外，我还考虑制作一个二级模型，该模型使用关于公司的新闻和Twitter的情绪值。我将首先概述如何获取这些数据。

Twitter数据
---------

除了股票价格数据，我还想尝试一些自然语言处理。因此，我尝试深入研究使用来自Twitter和新闻的情绪数据来改进股票预测。

第一个主要的斗争是免费获取推文，因为获取整个档案的Twitter API已经付清。但是，我找到了一个允许我在过去10天内获取推文的API，然后我可以实现某种形式的NLP来从推文中提取情绪数据。这不是最优的，但对我的在线学习模型仍然有用。

twitter API被用来刮掉过去10天，并且使用TextBlob计算情绪分数，并通过众多推文进行平均。

新闻数据
----

与Twitter类似，获取新闻数据非常困难。我尝试分析彭博文章的URL，但实现从2000年开始手动报废网站几乎是不可能的。因此，我选择了具有相当强大的抓取模型的Aylien API。

这些新闻文章被删除的条件是他们只包括股票和财经新闻，过滤到前150个Alexa网站，并且使用指数加权移动平均线平均情绪分数，以考虑最近的新闻而不是旧的新闻。

在线模型
----

鉴于我的情绪分数，我使用额外的神经网络层来更正我的预测误差。但是，在本文发布时无法获得此结果，因为生成一个数据点需要一天的时间。

结论
--

神经网络非常善于预测时间序列数据，并且当与情绪数据结合时，可以真正建立一个实用的模型。虽然这里的结果令人印象深刻，但我仍然想方设法改进它，也许实际上可以从中制定一个完整的交易策略。目前，我正在研究使用强化学习来开发一个使用预测模型结果的交易代理。