原文:![贝叶斯神经网络（LSTM）：实施](https://towardsdatascience.com/bayesian-neural-networks-lstm-3616327e8b7c)
**贝叶斯神经网络（LSTM）：实施**
====================

[![杰希尔帕里克](https://miro.medium.com/fit/c/96/96/0*JS-z3PP0581QIaH8.)](/@jehillparikh?source=post_page-----3616327e8b7c----------------------)

[杰希尔帕里克](/@jehillparikh?source=post_page-----3616327e8b7c----------------------)

跟随

[7月1日](/bayesian-neural-networks-lstm-3616327e8b7c?source=post_page-----3616327e8b7c----------------------) · 4 分钟阅读

贝叶斯推断允许量化不确定性，从而使得能够开发[稳健的机器学习模型](https://arxiv.org/pdf/1804.11313.pdf)。在实践中，需要采用采样方法来近似贝叶斯环境中遇到的后验分布/积分。对于在实践中典型使用的大型神经网络，采样方法是计算密集型的。已经开发了变分推断方法来克服这种限制。在之前的[帖子中](https://medium.com/p/788fd83f0e38/)，我们讨论了实用的变分推理算法的理论方面，Bayes的Back Prop（BBB）。通过放置权重的先验，BBB提供了一种减少认知不确定性的有效方法。认知不确定性，通常与缺乏训练数据有关。在帖子中，我们考虑BBB的实际实现，特别是对于LSMT。

![](https://miro.medium.com/max/60/1*VNBxJ4pVWhEDZ136Fc33gA.png?q=20)

![](https://miro.medium.com/max/1019/1*VNBxJ4pVWhEDZ136Fc33gA.png)

![](https://miro.medium.com/max/2038/1*VNBxJ4pVWhEDZ136Fc33gA.png)

**来自**[**Fortunato等人**](https://arxiv.org/pdf/1704.02798.pdf)**，2017年**

Tensorflow概率已经提供了标准的层这样的实现[致密](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational)，[卷积](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution1DReparameterization)。下面显示了一个示例模型网络，注意到API与Tensorflow API非常相似。因此，如果您需要使用这些标准层，请查看官方[文档](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/)。

![](https://miro.medium.com/max/60/1*DHbtDMfxI2_jVeOAOp4OaQ.png?q=20)

![](https://miro.medium.com/max/752/1*DHbtDMfxI2_jVeOAOp4OaQ.png)

![](https://miro.medium.com/max/1504/1*DHbtDMfxI2_jVeOAOp4OaQ.png)

**LSMT：**需要更多工作的一个例子是LSTM。LSTM是一类递归神经网络。[Colah的](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)博客很好地解释了他们。[此处](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)还提供了LSTM的逐步Tensorflow实现。如果您不确定LSTM的基础知识，我强烈建议您在继续学习之前先阅读它们。[Fortunato等人](https://arxiv.org/pdf/1704.02798.pdf)，2017年提供了贝叶斯LSTM的验证。原始源代码可用，不幸的是，我发现很难遵循。

**该实现包含LSTM的这四个方程**

![](https://miro.medium.com/max/60/0*xOBUVHALaAXd9q9v?q=20)

![](https://miro.medium.com/max/370/0*xOBUVHALaAXd9q9v)

![](https://miro.medium.com/max/740/0*xOBUVHALaAXd9q9v)

![](https://miro.medium.com/max/60/1*bRw08iQbQRVGCqURNBhonA.png?q=20)

![](https://miro.medium.com/max/691/1*bRw08iQbQRVGCqURNBhonA.png)

![](https://miro.medium.com/max/1382/1*bRw08iQbQRVGCqURNBhonA.png)

对于我们的权重，我们需要先验和变分后验。

我们通过为每个权重指定其（均值= 0，std = 1）来使用普通/高斯先验。

每个权重“w”的变分后验是通过效用函数获得的

![](https://miro.medium.com/max/60/1*AotLJtAJGq8ra7QUsr5Mog.png?q=20)

![](https://miro.medium.com/max/835/1*AotLJtAJGq8ra7QUsr5Mog.png)

![](https://miro.medium.com/max/1670/1*AotLJtAJGq8ra7QUsr5Mog.png)

此函数还计算这些权重的KL并将其添加到张量流集合中。该功能在[github](https://github.com/JP-MRPhys/bayesianLSTM)上可用。

为了实现贝叶斯LSTM，我们从tensorflow开始使用基础LSMT类，并通过在权重后面添加变量后覆盖调用函数，之后我们像往常一样计算门f，i，o，c和h。这非常简单，可以[在这里找到](https://github.com/JP-MRPhys/bayesianLSTM/blob/master/model/BayesianLSTM.py)。

这确保了我们拥有一致的tensorflow API，然后能够在此基础上构建我们的模型，例如，使用多层LSTM进行情感分析任务。请注意，结果测量的不确定性相当简单（二元）。

![](https://miro.medium.com/max/60/1*wovXyMKSUnetoUJVIz37-Q.png?q=20)

![](https://miro.medium.com/max/743/1*wovXyMKSUnetoUJVIz37-Q.png)

![](https://miro.medium.com/max/1486/1*wovXyMKSUnetoUJVIz37-Q.png)

在这里，我们定义贝叶斯LSTM层并通过单元格函数执行[展开](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。这种实现的一个缺点是不能使用tf.nn.dynamic\_rnn，因为它产生[while循环错误](https://stackoverflow.com/questions/56481239/multi-cell-lstm-rnn-while-loop-error-tensorflow-version-1-13-1)，与梯度检查有关。

然后我们可以获得logits和loss，注意我们使用[softmax层的](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax)变分后验[](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax)

![](https://miro.medium.com/max/60/1*6J--12Fu00MrcCSmpZWHCQ.png?q=20)

![](https://miro.medium.com/max/1237/1*6J--12Fu00MrcCSmpZWHCQ.png)

![](https://miro.medium.com/max/2474/1*6J--12Fu00MrcCSmpZWHCQ.png)

在训练中，我们获得了我们创建的每个变分后验的KL集合。这允许我们计算总KL损失，其除以批量大小以获得基于证据的下限，如[Fortunato等人](https://arxiv.org/pdf/1704.02798.pdf)的论文中所讨论的。

就是这样。其余代码只是标准数据输入和列车循环，模型保存等可以在[这里](https://github.com/JP-MRPhys/bayesianLSTM)找到[。](https://github.com/JP-MRPhys/bayesianLSTM)然后我们可以添加本文中提到的其他方面。