![雅可比正则化](https://towardsdatascience.com/jacobian-regularization-7db8836538cc)
雅可比正则化
======

L1和L2正则化的推广
-----------

[

![Michael Larionov，博士](https://miro.medium.com/fit/c/48/48/1*Yd6bDCYTVlArBqzlD8A0Hw.gif)

](/@michaellarionov?source=post_page-----7db8836538cc----------------------)

[Michael Larionov，博士](/@michaellarionov?source=post_page-----7db8836538cc----------------------)

跟随

[8月31日](/jacobian-regularization-7db8836538cc?source=post_page-----7db8836538cc----------------------) · 5 分钟阅读

![](https://miro.medium.com/max/30/0*HcHp9rciZXoLHaCF?q=20)

摄影：[Tom Mussak](https://unsplash.com/@tom_mu?utm_source=medium&utm_medium=referral)，[来自Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

L1和L2正则化，也称为Lasso和Ridge，是众所周知的正则化技术，用于各种算法。这些方法的想法是强加预测函数的平滑性并避免过度拟合。考虑[多项式回归的这个例子](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)：

![](https://miro.medium.com/max/30/1*C4dD5w-U2E0zZrmJQs_2bA.png?q=20)

![](https://miro.medium.com/max/1400/1*C4dD5w-U2E0zZrmJQs_2bA.png)

资料来源：[https](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)：[//scikit-learn.org/stable/auto\_examples/model\_selection/plot\_underfitting\_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)

在这个例子中，我们将不同度数的多项式拟合到数据中，其中高斯噪声被添加到函数中，在此标识为橙色曲线。我们看到，对于度数为15的多项式，预测曲线是高度非平滑的，并且模型不能很好地推广，与4阶的多项式模型相反，其中预测曲线非常接近原始曲线。

提高平滑度和避免过度拟合的一种方法是在成本函数中添加正则化项：

![](https://miro.medium.com/max/30/1*lSHJez5MP5XTRytG3ZB9HQ.png?q=20)

![](https://miro.medium.com/max/141/1*lSHJez5MP5XTRytG3ZB9HQ.png)

资料来源：[http](http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf)：[//statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf](http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf)

这里参数β通常排除截距。该正则化项将试图降低参数以改善平滑度。但它会以相同的力量拉下参数。但是在多项式回归的情况下，惩罚更高学位的术语比低学位术语更有意义。因此，当正方形的**β**被Tiknonov矩阵和向量**β**的乘积的平方范数代替时，L2正[则化](https://en.wikipedia.org/wiki/Tikhonov_regularization)的推广被称为[Tikhonov正则化](https://en.wikipedia.org/wiki/Tikhonov_regularization)。寻找Tiknonov矩阵的最佳值通常是困难的过程，尤其是对于参数数量巨大的神经网络。

为了解决这个问题，我们提出了一种新方法，在最近的一篇文章“用雅可比正则化进行稳健学习”中提供了一个适用于简单和复杂模型的正则化程序。

之前已经注意到，L2正则化等效于将高斯噪声添加到输入层。事实上，对于前馈网络，输入噪声始终比L2正则化提供更好的结果。本文的作者（以及其他一些论文）使用这个想法来推导出适用于简单和复杂模型的广义正则化项。我们的想法是，对于小扰动，我们可以使用logit函数的泰勒展开（在我们应用sigmod或softmax之前的最终层的输出）：

![](https://miro.medium.com/max/30/1*h7vNx5oxHMktIXdmqG_RQg.png?q=20)

![](https://miro.medium.com/max/815/1*h7vNx5oxHMktIXdmqG_RQg.png)

在这里我们介绍了雅可比**J**：

![](https://miro.medium.com/max/30/1*YYLUL_Tdvodwtj13Zhp2Kg.png?q=20)

![](https://miro.medium.com/max/215/1*YYLUL_Tdvodwtj13Zhp2Kg.png)

其中c是输出类（用于多类分类问题），i是输入的组成部分。正则化项将尝试最小化输入输出雅可比行列式的Frobenius范数：

![](https://miro.medium.com/max/30/1*crzzX3jV8Kovf_WONQLITA.png?q=20)

![](https://miro.medium.com/max/299/1*crzzX3jV8Kovf_WONQLITA.png)

因此，我们最小化的总成本函数如下所示：

![](https://miro.medium.com/max/30/1*73dlbnljveldU1xdgaeCkw.png?q=20)

![](https://miro.medium.com/max/641/1*73dlbnljveldU1xdgaeCkw.png)

JureSokolić，Raja Giryes，Guillermo Sapiro和Miguel RD Rodrigues在他们的论文_Robust大边缘深度神经网络中_早先提出了这种方法，但由于多类分类问题的计算复杂性，没有得到太多关注。本文通过对单位球上的随机方向进行采样来近似雅可比行列，从而克服了这些挑战。为了计算衍生物，我们可以使用所有现代深度学习框架中可用的自动差异。

![](https://miro.medium.com/max/30/1*ZhcnAxpFD25exAmrLDUx_w.png?q=20)

![](https://miro.medium.com/max/948/1*ZhcnAxpFD25exAmrLDUx_w.png)

我们可以将类似的方法应用于我们在开始时讨论的多项式回归。我们将曲线建模为多项式函数：

![](https://miro.medium.com/max/30/1*wOZ4VtdM7ryq2Cj01d2a3A.png?q=20)

![](https://miro.medium.com/max/210/1*wOZ4VtdM7ryq2Cj01d2a3A.png)

这里d是多项式的次数。在这种情况下雅可比矩阵是1x1矩阵：

![](https://miro.medium.com/max/30/1*KnZr25WsMaRYAl68zXzbsA.png?q=20)

![](https://miro.medium.com/max/275/1*KnZr25WsMaRYAl68zXzbsA.png)

然后正则化项可以写成：

![](https://miro.medium.com/max/30/1*lLlk4FT3erpUv2ecCudnjQ.png?q=20)

![](https://miro.medium.com/max/267/1*lLlk4FT3erpUv2ecCudnjQ.png)

在这里，我们总结了观察结果。注意，这将是a的复数二次函数，这可能与简单的L2正则化函数非常不同。另请注意，此估算器可能存在偏差，并且必须应用某种贝塞尔校正。本文尚未对此进行讨论，但估计偏差并进一步改善结果将会很有趣。

可以预见，如果我们将函数建模为1阶多项式（这将使其成为线性回归），则正则化项将变为：

![](https://miro.medium.com/max/30/1*ELLPNQx4CWgzflK3MFYF3Q.png?q=20)

![](https://miro.medium.com/max/116/1*ELLPNQx4CWgzflK3MFYF3Q.png)

这正是L2正规化！

结论
==

神经网络的正则化仍然是一个活跃的研究领域，我们回顾的论文以及其他几篇论文继续推进它，提高准确性，同时也为正规化技术奠定了坚实的理论基础。雅可比正则化不仅可以应用于神经网络，还可以应用于其他算法，例如，用于多项式回归。