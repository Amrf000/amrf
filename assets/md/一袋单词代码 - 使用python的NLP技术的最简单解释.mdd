![一袋单词代码 - 使用python的NLP技术的最简单解释](https://medium.com/swlh/bag-of-words-code-the-easiest-explanation-of-nlp-technique-using-a-python-8a4fdfb8598c)
一袋单词代码 - 使用python的NLP技术的最简单解释
=============================

[![罗希特·马丹](https://miro.medium.com/fit/c/48/48/2*i85v8QSMFqvZLkpp0E8bOA.jpeg)](/@madanflies?source=post_page-----8a4fdfb8598c----------------------)

[罗希特·马丹](/@madanflies?source=post_page-----8a4fdfb8598c----------------------)

跟随

[5月30日](/swlh/bag-of-words-code-the-easiest-explanation-of-nlp-technique-using-a-python-8a4fdfb8598c?source=post_page-----8a4fdfb8598c----------------------) · 4 分钟阅读

阿罗哈我的同伴乘客，_（跳过结束代码）_

今天我要向你解释Bag of words技术。

如果你在这里，你可能知道我们为什么使用它但如果你不知道那么我会告诉你一个例子。

用一个例子解释一堆词？
===========

转到您的Gmail，打开优先收件箱，并通过对所有电子邮件的重要，社交，垃圾邮件等进行分类，在您的收件箱中查看Google魔术。

![](https://miro.medium.com/max/30/1*e9hJGnnjXSQRoXx7viEAFw.png?q=20)

![](https://miro.medium.com/max/2130/1*e9hJGnnjXSQRoXx7viEAFw.png)

还记得吗？

谷歌如何知道一些邮件对你很重要，而其他邮件则不然？

虽然有许多因素可以使用，但一个主要因素是机器读取你的邮件，然后了解对你来说重要的事情，然后向你展示，Voila。

问题是，机器不知道什么是英语，它只能理解数字，所以它的作用是将所有文件分解为单词，例如 -

**电子邮件**

> 嗨马丹先生，
> 
> 恭喜，我喜欢你的文章，因为它能够以简单的方式向我解释一下单词的内容。

**机器如何打破它**

> 嗨，先生，Madan，恭喜，我，爱，你的，文章............ in，a，简单，方式。

**这种分裂的话就是** -

1.  **清理或预处理 -**删除所有不必要的特殊字符，如果有其他口音的字样，如波兰语，德语，西班牙语等，请删除或替换它们或添加正确的unicode以使其对机器可读。
2.  **规范化所有数据** - 使用.lower（）函数从数据中删除任何大写字母。
3.  **Lamentization和steming -**删除所有形容词/建立在数据上的单词，即烘烤，烘焙，面包师都是建立在烘烤的单词上。将所有此类单词从数据分类到根单词。此外，删除所有停止词，即所有不添加任何意义或维度的词，如a，the等。

接下来，一旦我们拥有了所有单词，我们就会对其进行标记，即写入文档中所有单词的重复计数。前 - 这只猫是可爱的肥猫>>这个，猫，是，脂肪，猫，那，是，可爱>>>这个 - 1，猫 - 2，是-2，一个-1，脂肪 - 1，那-1，可爱 - 1

要么

这 - 1

猫 - 2

是-2

a - 1

脂肪 - 1

\-1

可爱 - 1

此过程称为**标记化。**
-------------

在我们了解接下来要做什么之前，我会告诉你为什么我们要做这一切。

> 目标是在文档中查找特征或单词，以帮助我们从文档中提供某些含义或在与类似的非类似文档进行比较时提供一些帮助。

分解文档1的标记化单词有助于我们将其与文档2中的其他标记化单词进行比较，从而有助于查找类似文档。

**现在又回到了包里。**

在标记化之后，我们转向构建词汇表或从文档中查找功能。

vocab =清洁后的所有最终功能，删除停用词等。

因此，如果一个文档有3个文档，每个文档有7个单词，那么词汇是文档中单词的最佳选择，在我们的例子中是21个单词中的10个单词。

词汇计数=我们所有的独特特征或单词的数量（Say）。

在我们找到词汇后，我们将最终确定的词转换为向量，你怎么问？

假设我们的词汇_“这猫是肥猫是可爱”_是

猫 - 2

是 - 2

脂肪 - 1

可爱 - 1

词汇数= 4

文件的载体_“这只猫是可爱的肥猫”_是

\[022010001\]
-------------

当你比较文件中的最终词汇与单词时，这个数字只不过是一种表示

这个 - 1，猫 - 2，是-2，一个-1，脂肪 - 1，那个-1，可爱 - 1

和猫 - 2，是 - 2，脂肪 - 1，可爱 - 1

我们得到**\[022010001\]**

此过程称为**矢量化。**

Tada，这些都是包装词的概念，所以现在就像我承诺的那样，我正在与你分享我的代码，这是基于sklearn的，如果你想看看每个过程如何通过代码工作，[**请检查一下**](https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/)。

* * *

**_#Part 1声明所有_**文档**_并分配给文档_**  
document1 =“这是我的文字袋上的代码。”  
document2 =“一袋字是一种NLP技术。”  
document3 =“我会以简单的方式向你解释”

**_#Part 2 - 导入库并初始化CountVectorizer_**

来自sklearn.feature\_extraction.text导入CountVectorizer

＃从所有文档中制作列表或文档  
Doc = \[document1，document2，document3\]

#Initializing来自sklearn  
vectorizer的CountVectorizer \= CountVectorizer（stop\_words ='english'）

**_#Part 3 - 获取我们将用于标记此文档的最终单词的功能名称_**

X = vectorizer.fit\_transform（Doc）

analyze = vectorizer.build\_analyzer（）

分析（文档1）

分析（书2）

分析（文件3）

vectorizer.transform（文档1）.toarray（）

vectorizer.transform（文件2）.toarray（）

vectorizer.transform（文件3）.toarray（）

打印（vectorizer.get\_feature\_names（））

_输出_ >>> _\['bag'，'code'，'explain'，'nlp'，'simple'，'technique'，'way'，'words'\]_

**_#Part 4 - 矢量化或创建所有三个_**文档**_的_****_矩阵_**

打印（X.toarray（））

_输出_ >>> \[\[1 1 0 0 0 0 0 1\] \[1 0 0 1 0 1 0 1\] \[0 0 1 0 1 0 1 0\]\]

要么

去结帐我的Github在这里> [**检查包的单词代码**](https://github.com/rmadan16/Bag_of_words)。

我希望这很简单并帮助您理解这个概念，如果您有反馈可以帮助我改进内容或代码，请写信给我 - _rohitmadan16@gmail.com_

和平。