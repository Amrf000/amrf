![DCGANs - 使用Tensorflow和Keras生成狗图像](https://towardsdatascience.com/dcgans-generating-dog-images-with-tensorflow-and-keras-fb51a1071432)
DCGANs - 使用Tensorflow和Keras生成狗图像
================================

[![Konstantin Georgiev](https://miro.medium.com/fit/c/48/48/2*2HhmAkgYHmPjd2Ki6XqA5g.jpeg)](/@dragonflareful?source=post_page-----fb51a1071432----------------------)

[Konstantin Georgiev](/@dragonflareful?source=post_page-----fb51a1071432----------------------)

跟随

[8月30日](/dcgans-generating-dog-images-with-tensorflow-and-keras-fb51a1071432?source=post_page-----fb51a1071432----------------------) · 23 分钟阅读

基于Kaggle的Generative Dogs比赛（2019年）
=================================

时代的DCGAN模型可视化（约9小时的训练）

这篇文章是关于**DCGAN**有效性背后的基本思想的教程，以及一些改善其性能的方法/黑客。这些方法都是基于我在**Kaggle的**[**Generative Dogs Competition中的**](https://www.kaggle.com/c/generative-dog-images)经验**。该教程也可以在我的原始**[**内核**](https://www.kaggle.com/jadeblue/dcgans-and-techniques-to-optimize-them)**上以笔记本格式****或在**[**GitHub上使用**](https://github.com/JadeBlue96/DCGAN-Dog-Generator)**。**

要在本地或使用**Colab**运行此示例，您需要一个**Kaggle帐户**，以便检索其API密钥并使用提供的数据集。[**这里有**](https://medium.com/@yvettewu.dw/tutorial-kaggle-api-google-colaboratory-1a054a382de0)完整的教程。

生成对抗网络（GAN）
===========

与大多数流行的神经网络架构不同，**GAN**被训练为同时解决两个问题 - **辨别**（有效地分离真实图像）和**“逼真”伪造数据生成**（有效地生成被认为是真实的样本）。我们可以看到这些任务是完全相反的，但如果我们将它们分成不同的模型会发生什么？

那么，这些模型的通用名称是**Generator（G）**和**Discriminator（D）**，被认为是**GAN**理论背后的**基石**。

所述**发电机**网络需要输入一个简单随机噪声N维向量，并且根据学习目标分布转换它。它的输出也是N维的。另一方面，**鉴别器**模拟概率分布函数（如分类器）并输出输入图像是真实的还是假的**\[0,1\]**的概率。考虑到这一点，我们可以定义生成任务的两个主要目标：

**1.训练G以最大化D的最终分类错误。**（这样生成的图像被认为是真实的）。

**2.训练D以最小化最终的分类错误。**（这样可以将真实数据与假数据正确区分开来）。

为了实现这一点，在**反向传播**期间，**G**的权重将使用**渐变上升**来更新以最大化误差，而**D**将使用**渐变下降**来最小化它。

![](https://miro.medium.com/max/30/0*efj9Bgi86fdiyfDq.png?q=20)

![](https://miro.medium.com/max/1132/0*efj9Bgi86fdiyfDq.png)

GAN输入和输出 - （请注意，两个网络在训练期间不直接使用图像的真实分布，而是使用彼此的输出来估计其性能）

那么我们如何定义估算两个网络累积性能的**损失函数**呢？好吧，我们可以使用**绝对误差**来估计**D**的误差，然后我们可以为**G**重复使用相同的函数但是最大化：

![](https://miro.medium.com/max/30/1*Ip0v_LDojrOQTX3ikMojNg.png?q=20)

![](https://miro.medium.com/max/532/1*Ip0v_LDojrOQTX3ikMojNg.png)

平均绝对误差 - 图像的真实和假分布之间的距离

在这种情况下，**_p\_t_**表示图像的真实分布，而**_p\_g_**是从**G**创建的分布。

我们可以观察到这个理论是基于**强化学习的**一些关键概念。它可以被认为是一个双人迷你极小游戏，两个玩家相互竞争，从而逐步改善他们各自的任务。

我们看到了**GAN**理论背后的基本思想。现在让我们更进一步，通过应用**卷积神经网络的**思想和方法，了解**DCGAN的**工作原理。

深度卷积生成对抗网络（DCGAN）
=================

**DCGAN**利用**CNN的**一些基本原理，并因此成为实践中使用最广泛的架构之一，因为它们快速收敛，并且由于它们可以非常容易地适应更复杂的变体（使用标签作为条件） ，应用残差​​块等）。以下是**DCGAN**解决的一些更重要的问题：

*   **创建D使得它基本上解决了受监督的图像分类任务。**（对于这种情况下狗或没有狗）
*   **由GAN学习的过滤器可用于在生成的图像中绘制特定对象。**
*   **G包含矢量化属性，可以学习非常复杂的对象语义表示。**

以下是创建稳定**DCGAN**时要考虑的一些核心准则，而不是标准**CNN**（摘自官方[文件](https://arxiv.org/pdf/1511.06434.pdf)）：

*   **用Strided convolutions替换Pooling函数。**（这允许**D**学习它自己的空间下采样和**G**各自的上采样，而不会给模型增加任何偏差）
*   **使用BatchNorm**（它通过将每个单元的输入标准化为零均值和单位方差来稳定学习，这也有助于创建更强大的深度模型而不会使梯度发散）
*   **避免使用完全连接的隐藏层（不输出）。**（例如，这是全球平均合并，似乎会损害收敛速度）
*   **对于G - 使用ReLU激活和Tanh输出。**（当您将图像作为输出时，tanh通常是更优先的激活，因为它具有\[-1,1\]的范围）
*   **对于D - 使用LeakyReLU激活（以及输出概率的sigmoid函数）。**（这是根据经验进行测试，似乎可以很好地建模到更高的分辨率）

以下是**DCGAN发生器**的最标准结构：

![](https://miro.medium.com/max/30/1*QsYQLn7wfIIeZ49XU7SLuw.png?q=20)

![](https://miro.medium.com/max/875/1*QsYQLn7wfIIeZ49XU7SLuw.png)

DCGAN发生器结构

我们可以观察到，它的初始输入只是一个**（1,100）**噪声向量，它通过**4个卷积**层进行上采样，步长为**2**，产生大小为（64,64,3）的RGB图像。为实现此目的，将输入向量投影到1024维输出上以匹配第一个**Conv**层的输入，稍后我们将看到更多。

标准鉴别器会是什么样子？好吧，关于你的期望，让我们来看看：

![](https://miro.medium.com/max/30/1*bjMzwBibh-KAb_06QV32bA.png?q=20)

![](https://miro.medium.com/max/325/1*bjMzwBibh-KAb_06QV32bA.png)

DCGAN鉴别器结构

这次我们有一个输入图像**（64,64,3）**，与**G**的输出相同。我们将其传递给**4个标准的下采样Conv层**，再次以**2**的步幅。在最终输出图层中，图像被平展为矢量，该矢量通常被馈送到S形函数，然后输出**D**对该图像的预测**（单个值表示范围\[0,1\]中的概率 - 狗= 1或没有狗= 0）**。

那么，现在你看到了**GAN**和**DCGAN**背后的基本思想，所以现在我们可以继续使用**Tensorflow和Keras** :) 生成一些狗。

图像预处理和EDA（探索性数据分析）
==================

在我们继续创建**GAN**模型之前，让我们先快速浏览一下我们将要使用的**Stanford Dogs**数据集。因为我们还有每个图像的注释，我们可以使用它们将每个狗图像映射到其各自的品种。为此，我们可以先创建一个字典，将文件名中的品种代码映射到实际的品种名称。

接下来，我将使用**OpenCV**来快速读取图像并将其转换为**RGB**。

如果我们查看数据集，我们可以看到每个注释文件夹都包含一个**xml**文件列表。这些文件与特定图像相关联，并包含非常有用的信息，主要是图像中每只狗周围的边界框。还有一些图像中包含多条狗，这使我们可以准确地裁剪它们并制作一个**仅**包含**单只狗**图像的数据集。

在这里，我们可以利用**xml**库创建一个树，并找到该注释的相关元素。对于每个对象，我们可以提取边界框坐标，裁剪图像并通过根据结果**图像宽度****缩小**或**扩展**它来标准化裁剪。最后，我们将把图像保存在一个numpy数组中。

使用ET查找图像中每只狗的注释

for o in objects：   
 bndbox = o.find（'bndbox'）   
 xmin = int（bndbox.find（'xmin'）。text）   
 ymin = int（bndbox.find（'ymin'）。text）   
 xmax = int（bndbox .find（'xmax'）。text）   
 ymax = int（bndbox.find（'ymax'）。text）..添加一些边距并调整宽度..＃裁剪图像   
 img\_cropped = img \[ymin：ymin + w， xmin：xmin + w，：\]＃\[h，w，c\] ..插值步骤..   
＃调整图像大小   
 img\_cropped = cv2.resize（img\_cropped，（image\_width，image\_height），interpolation = interpolation）  
＃保存图像和标签  
dog\_images\_np \[curIdx，：，：，\]\] = np.asarray（img\_cropped）  
dog\_breed\_name = dog\_breed\_dict \[dog\_ann.split（' \_'）\[0\]\]   
breeds.append（dog\_breed\_name）  
curIdx + = 1   
                  
返回dog\_images\_np，品种

_这些功能需要大约2-3分钟才能加载。_

通过试验，我们可以发现只有单只狗的结果图像是**22125**，因此我们可以指定我们的numpy数组的确切大小。有**120**种不同的犬种。

现在我们有了特征和标签，我们可以将它们绘制成方形网格，以查看作物的外观并确保它们被正确标记。

def plot\_features（features，labels，image\_width = image\_width，image\_height = image\_height，image\_channels = image\_channels，  
examples = 25，disp\_labels = True）：  
    
 如果不是math.sqrt（例子）.is\_integer（）：  
 print（'请选择有效数字示例。'）  
 返回  
 imgs = \[\]   
 classes = \[\]   
 for i in range（examples）：  
 rnd\_idx = np.random.randint（0，len（labels））  
 imgs.append（features \[rnd\_idx，：，：，\]\] ）  
 classes.append（labels \[rnd\_idx\]） fig，axes = plt.subplots（round（math.sqrt（examples）），round（math.sqrt（examples）），figsize =（15,15），  
 subplot\_kw = {' xticks'：\[\]，'yticks'：\[\]}，  
 gridspec\_kw = dict（hspace = 0.3，wspace = 0.01））  
      
 for i，ax in enumerate（axes.flat）：  
 if disp\_labels == True：  
 ax.title.set\_text（classes \[i\]）  
 ax.imshow（imgs \[i\]）

请注意，我们需要对像素值进行标准化，以确保正确绘制狗。

plot\_features（dog\_images\_np / 255.，品种，示例= 25，disp\_labels = True）

![](https://miro.medium.com/max/30/1*9LwQfdpKPB_OvuQq92YMlg.png?q=20)

可视化图像功能

模型超参数列表
=======

在这里，我们有一个完整的超参数列表，您可以调整和使用它们来尝试改进模型。我主要从研究论文中收集这些价值观，并通过调整它们进行了一些实验，这就是我最终的结果。以下是您可以尝试的事项列表：

*   **样本大小** - 功能的数量
*   **批量大小** - 64或32可以提高性能，但计算量很大，你只能运行模型少量的时期
*   **Weight Init Std和Mean** - 这些值来自研究论文，似乎可以稳定模型训练
*   **Leaky ReLU斜率** - **D**激活的门槛也似乎很强劲
*   **缩小因子和比例因子** - 设置使得**G**的噪声向量可以重新**变形为（4,4,512**），其他组合也可以起作用
*   **辍学** - 辍学层的数量，它们的位置和速率可以提高性能。
*   **学习率和学习率衰减** - 对模型融合非常重要，难以精确调整，**G**和**D**可以有不同的学习率。
*   **噪声矢量形状** - 通常128或100似乎就足够了

创建图像数据集
=======

现在让我们使用我们的numpy特性数组来构造**Tensorflow**数据集对象。首先，我们可以将数据类型转换为**float32**，这总是有助于保留一些内存。

dog\_features\_tf = tf.cast（dog\_images\_np，'float32'）

我们还可以将**数据增强**应用于我们的数据集。这包括随机**水平翻转**，缩放和裁剪随机区域中的图像。在这些中，我发现只有第一种方法对于为数据集添加更多方差有些用处，因为其他方法会引入大量噪声。

因此，在这种情况下，我们的数据集中的图像将有**50％的**可能性从左向右翻转。

def flip（x：tf.Tensor） - >（tf.Tensor）：  
 x = tf.image.random\_flip\_left\_right（x）  
 return x

现在我们可以使用**Tensorflow**通过对其进行混洗，应用一些扩充，最后将其分成指定**批量大小的**批次来创建数据集。

dog\_features\_data = tf.data.Dataset.from\_tensor\_slices（dog\_features\_tf）.shuffle（sample\_size）.map（flip）.batch（batch\_size，drop\_remainder = True）

规范化技术
=====

在我们实际**生成Generator**之前，让我们看一些可以逐步加速**DCGAN**收敛的**规范化**。

重量初始化
=====

这些方法之一是权**重初始化**。事实证明，这对培养稳定的**GAN**非常重要。首先，模型权重需要以**零为中心**，并略微增加**std**（0.02）。这在训练期间稳定了**D**和**G**，并防止模型梯度消失或爆炸。这是每种情况下的关键步骤，我们必须在模型中使用随机变量（随机噪声向量）。

以下是权**重初始化**如何严重影响神经网络学习过程的示例。

![](https://miro.medium.com/max/30/1*PaTir4OPRGRMpWCCwv6eXQ.png?q=20)

![](https://miro.medium.com/max/1370/1*PaTir4OPRGRMpWCCwv6eXQ.png)

权重初始化对模型训练的影响

我们还可以使用**Keras**应用**截断正态分布**，这将丢弃**与平均值****相差****超过2个标准偏差的值**。这可能会在培训期间消除一些异常点。

weight\_initializer = tf.keras.initializers.TruncatedNormal（stddev = weight\_init\_std，mean = weight\_init\_mean，seed = 42）

光谱归一化
=====

**Spectral Normalization**是一种新型的权重初始化，专为**GAN**设计，似乎可以进一步稳定模型训练（您可以从[本文中](https://arxiv.org/pdf/1802.05957.pdf)阅读更多内容）。有关**光谱归一化**及其工作原理的更详细说明，也值得查看这篇[文章](https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html)，它有非常直观的例子。

**光谱**我们网络中单个权重的**标准化**可以定义如下：

![](https://miro.medium.com/max/30/1*szrUCXyXqK5uIk1xUxKvfQ.png?q=20)

对单个权重应用光谱归一化

这里_u_和_v_是相同大小的简单随机向量。对于每个学习步骤，它们被用于对特定权重执行所谓的**功率迭代**操作，并且证明它比简单地惩罚梯度更具计算效率。

此后，在反向传播的步骤，我们使用_无线传感器网络（W）_更新权重，而不是_w ^_。

对于这个项目，我会重用一些定制**Keras**由执行层**IShengFang**（[官方代码](https://github.com/IShengFang/SpectralNormalizationKeras)），应用**谱归**之上**转化率**和**致密**层。

这也是**光谱归一化**效果的一个很好的例子：

![](https://miro.medium.com/max/30/1*rd7XVN8zTgZB3gGgqFkC6w.png?q=20)

![](https://miro.medium.com/max/640/1*rd7XVN8zTgZB3gGgqFkC6w.png)

SN的DCGAN培训

![](https://miro.medium.com/max/30/1*h80WCznlsGXifDDxrbUAow.png?q=20)

![](https://miro.medium.com/max/640/1*h80WCznlsGXifDDxrbUAow.png)

使用标准层进行DCGAN培训

我们还在**Keras中**定义了一些模板层，以便我们以后可以更轻松地创建**G**和**D.** 图层的标准模式是：

**TP\_Conv\_Block = \[（Conv（SN）2DTranspose（upsample）） - >（BatchNorm） - >（ReLU）\]**

**Conv\_Block = \[（Conv（SN）2D（下采样）） - >（BatchNorm） - >（LeakyReLU）\]**

对于这个例子，我将使用**D中的G和Spectral Normalized Conv2D层中的**标准**Conv2DTranspose块。**

def transposed\_conv（model，out\_channels，ksize，stride\_size，ptype ='same'）：  
 model.add（Conv2DTranspose（out\_channels，（ksize，ksize），  
 strides =（stride\_size，stride\_size），padding = ptype，  
 kernel\_initializer = weight\_initializer，use\_bias = False））  
 model.add（BatchNormalization（））  
 model.add（ReLU（））  
 返回模型  
  
def convSN（model，out\_channels，ksize，stride\_size）：  
 model.add（ConvSN2D（out\_channels，（ksize，ksize），strides =（ stride\_size，stride\_size），padding ='same'，  
 kernel\_initializer = weight\_initializer，use\_bias = False））  
 model.add（BatchNormalization（））  
 model.add（LeakyReLU（alpha = leaky\_relu\_slope））  
 ＃model.add（Dropout（dropout\_rate））  
 返回模型  

发电机
===

我们终于可以定义我们的发电机 模型结构主要基于官方的DCGAN [论文，](https://arxiv.org/pdf/1511.06434.pdf)并进行了一些调整，我认为这些调整对性能有益。这是整体结构：

**\[输入（128,1） - >密集（2048，） - >重塑（4,4,128） - > TP\_Conv\_Block（深度= 512，K = 5x5，S = 1x1） - >丢失（0.5） - > TP\_Conv\_Block（深度= 256，K = 5x5，S = 2x2） - >丢失（0.5） - > TP\_Conv\_Block（深度= 128，K = 5x5，S = 2x2） - > TP\_Conv\_Block（深度= 64，K = 5x5，S = 2x2） - > TP\_Conv\_Block（深度= 32，K = 5x5，S = 2x2） - >密集（深度= 3，Tanh）\]**

def DogGenerator（）：  
 model = Sequential（）  
 model.add（Dense（image\_width // scale\_factor \* image\_height // scale\_factor \* 128，  
 input\_shape =（noise\_dim，），kernel\_initializer = weight\_initializer））  
 ＃model.add（BatchNormalization（epsilon = BN\_EPSILON） ，momentum = BN\_MOMENTUM））  
 ＃model.add（LeakyReLU（alpha = leaky\_relu\_slope））  
 model.add（Reshape（（image\_height // scale\_factor，image\_width // scale\_factor，128）））  
      
 model = transposed\_conv（model，512，ksize = 5 ，stride\_size = 1）  
 model.add（Dropout（dropout\_rate））  
 model = transposed\_conv（model，256，ksize = 5，stride\_size = 2）  
 model.add（Dropout（dropout\_rate））  
 model = transposed\_conv（model，128，ksize = 5 ，stride\_size = 2）  
 model = transposed\_conv（model，64，ksize = 5，stride\_size = 2）  
 model = transposed\_conv（model，32，ksize = 5，stride\_size = 2）  
      
 model.add（Dense（3，activation ='tanh'，kernel\_initializer = weight\_initializer） ）  
  
 返回模型

鉴别者
===

该**鉴别**相对容易实现，因为它基本上是一个小**CNN**两级分类。我们可以选择是否应用**光谱归一化**，并查看性能效果。对于这个例子，我将尝试仅在**D中**应用**SN**。这是**D**的结构：

**\[输入（128,128,3） - > Conv（SN）2D（深度= 64，K = 5x5，S = 1x1，相同） - > LeakyReLU - > Conv\_Block（深度= 64，K = 5x5，S = 2x2） - > Conv\_Block（深度= 128，K = 5x5，S = 2x2） - > Conv\_Block（深度= 256，K = 5x5，S = 2x2） - >展平 - >密集（16384，Sigmoid）\]**

另请注意，所有**Conv**和**Dense**图层都使用上面定义的**Truncated Normal**分布进行初始化。另一件事是**偏置**项从**Conv**层中移除，这也使模型稍微稳定一些。

def DogDiscriminator（spectral\_normalization = True）：  
 model = Sequential（）  
 if spectral\_normalization：  
 model.add（ConvSN2D（64，（5,5），strides =（1,1），padding ='same'，use\_bias = False，  
 input\_shape = \[image\_height，image\_width，image\_channels\]，  
 kernel\_initializer = weight\_initializer））  
 ＃model.add（BatchNormalization（epsilon = BN\_EPSILON，momentum = BN\_MOMENTUM））  
 model.add（LeakyReLU（alpha = leaky\_relu\_slope））  
 ＃model.add（Dropout（dropout\_rate））  
          
 model = convSN（model，64，ksize = 5，stride\_size = 2）  
 #model = convSN（model，128，ksize = 3，stride\_size = 1）  
 model = convSN（model，128，ksize = 5，stride\_size = 2）  
 #model = convSN（model，256，ksize = 3，stride\_size = 1）  
 model = convSN（model，256，ksize = 5，stride\_size = 2）  
 #model = convSN（model，512，ksize = 3，stride\_size = 1）  
 ＃model.add（Dropout（dropout\_rate））  
  
 model.add（Flatten（））  
 model.add（DenseSN（1，activation ='sigmoid'））  
 else：  
 ...   
 return model dog\_discriminator = DogDiscriminator（spectral\_normalization = True）

标签平滑
====

可以在训练期间应用的一种正则化方法称为**标签平滑**。这样做是因为它基本上可以防止**D**在其预测中过于自信或不自信。如果**D**变得过于确定特定图像中存在狗，则**G**可以利用该事实并且不断地开始仅生成该类型的图像，并且反过来，停止改进。我们可以通过将类标签设置为负类的**\[0,0.3** **\]**范围和正类的**\[0.7,1\]**来解决这个问题。

这将防止整体概率非常接近两个阈值。

＃标签平滑 - 来自GAN黑客的技术，而不是将1/0指定为类标签，我们为正类  
＃和\[   
  
0.0,0.3 \] 分配范围\[ 0.7,1.0 \] 的随机整数，用于负类def smooth\_positive\_labels（y） ：  
 return y  -  0.3 +（np.random.random（y.shape）\* 0.5）  
  
def smooth\_negative\_labels（y）：  
 return y + np.random.random（y.shape）\* 0.3

向标签引入噪音
=======

此技术也称为**实例噪声**。通过向标签添加少量误差（假设为5％），这往往会使真实和预测的分布更加分散，从而开始相互重叠。这反过来使得在学习过程中更容易拟合生成的图像的自定义分布。

以下是使用这些技术如何使用这两种发行版的一个很好的示例：

![](https://miro.medium.com/max/30/1*EA95dVM9vFHBTxWjrwYJCQ.png?q=20)

![](https://miro.medium.com/max/1711/1*EA95dVM9vFHBTxWjrwYJCQ.png)

Insance Noise和Label平滑对图像的真实和虚假分布的影响

以下是我们如何实现**实例噪声**：

＃随机翻转一些标签  
def noisy\_labels（y，p\_flip）：  
 ＃确定要翻转的标签数量  
 n\_select = int（p\_flip \* int（y.shape \[0\]））  
 ＃选择要翻转的标签  
 flip\_ix = np.random.choice（ \[i for i in range（int（y.shape \[0\]））\]，size = n\_select）  
      
 op\_list = \[\]   
 ＃反转标签  
 #y\_np \[flip\_ix\] = 1  -  y\_np \[flip\_ix\]   
 for i in range（ int（y.shape \[0\]））：  
 如果我在flip\_ix中：  
 op\_list.append（tf.subtract（1，y \[i\]））  
 else：  
 op\_list.append（y \[i\]）  
      
 outputs = tf.stack（op\_list ）  
 返回输出

优化器
===

这项任务的最佳验证优化算法是**Adam**，两种模型的标准学习率为**0.0002**，beta为**0.5**。

generator\_optimizer = tf.train.AdamOptimizer（learning\_rate = lr\_initial\_g，beta1 = 0.5）  
discriminator\_optimizer = tf.train.AdamOptimizer（learning\_rate = lr\_initial\_d，beta1 = 0.5）

定义损失函数
======

最近优化**GAN的**另一个新趋势是应用**相对论**损失函数而不是标准函数。这些函数测量实际数据比生成数据更“真实”的概率。一种比较流行的相对论函数选择包括**RaLSGAN（相对论平均最小二乘法），RaSGAN（相对论平均标准）和RaHinge（相对论铰链损失）**。

但在此之前，让我们定义标准的**GAN**损失：

![](https://miro.medium.com/max/30/1*o8VrOm7QexoDjOOoi_sHpg.png?q=20)

![](https://miro.medium.com/max/598/1*o8VrOm7QexoDjOOoi_sHpg.png)

我们可以观察到，这基本上是用于分类任务的标准**二进制Crossentropy损失**或实际和生成分布之间的**Logistic损失**。在**Tensorflow中**，这可以定义如下：

相比之下，这是**RSGAN（相对论标准）**损失的样子：

![](https://miro.medium.com/max/30/1*vrDKykPN7Fwny5ffWvV6Ew.png?q=20)

![](https://miro.medium.com/max/562/1*vrDKykPN7Fwny5ffWvV6Ew.png)

在这种情况下，任务是不同的，以测量**真实（r）**和**假（f）**数据分布之间的相似性。当D（x）= 0.5（即_C_（_xr_）= _C_（_xf_））时，RSGAN达到最佳点。存在许多相对论的损失函数变体，它们都包含用于测量这种相似性的不同方法。在这个项目中，我尝试了**3**种似乎具有最佳记录的**MIFID**分数的变体**（RaLSGAN，RaSGAN和RaHinge）**。随意尝试不同的损失，看看你是否可以提高性能;）。

以下是最常用的列表：

![](https://miro.medium.com/max/17/1*R6cGS9l48aQcT9SEDvLFlA.png?q=20)

![](https://miro.medium.com/max/875/1*R6cGS9l48aQcT9SEDvLFlA.png)

标准和相对论的GAN损失

在针对这一特定问题的许多试验中，我没有发现通过切换到**相对论性**损失来提高性能，所以我决定坚持使用标准的**GAN**损失函数，因为它更容易估计，尽管在某些情况下这些损失可以真正加快模型的收敛速度。

以下是使用标准**平滑和实例噪声**时**鉴别器**损失函数的样子。它基本上是两个子损失的总和**（假 - 根据G的图像，真实 - 就实际训练图像而言）**。

def discriminator\_loss（real\_output，fake\_output，loss\_func，apply\_label\_smoothing = True，label\_noise = True）：  
 if label\_noise和apply\_label\_smoothing：  
 real\_output\_noise = noisy\_labels（tf.ones\_like（real\_output），0.05）  
 fake\_output\_noise = noisy\_labels（tf.zeros\_like（fake\_output），0.05）  
 real\_output\_smooth = smooth\_positive\_labels（real\_output\_noise）  
 fake\_output\_smooth = smooth\_negative\_labels（fake\_output\_noise）  
 if loss\_func =='gan'：  
 real\_loss = cross\_entropy（tf.ones\_like（real\_output\_smooth），real\_output）  
 fake\_loss = cross\_entropy（tf.zeros\_like（fake\_output\_smooth），fake\_output） else：  
...其他损失函数变体 loss = fake\_loss + real\_loss   
 回损

并且**发电机**损耗功能**（应用标签平滑）**只是标准的**物流损失**：

def generator\_loss（real\_output，fake\_output，loss\_func，apply\_label\_smoothing = True）：  
 if apply\_label\_smoothing：  
 fake\_output\_smooth = smooth\_negative\_labels（tf.ones\_like（fake\_output））  
 if loss\_func =='gan'：  
 return cross\_entropy（tf.ones\_like（fake\_output\_smooth），fake\_output）  
 else：  
...其他损失函数变量 返回损失

主要训练循环
======

我们还要确定训练时期的数量和要提供给**发生器**的图像数量，以便可视化中间结果。

EPOCHS = 280   
num\_examples\_to\_generate = 64   
seed = tf.random.normal（\[num\_examples\_to\_generate，noise\_dim\]）

**DCGAN的**一个培训步骤包括三个标准步骤：

1.  **前方道具** - **G**创造了一批假图像; 此，沿着一批真实图像被馈送到**d**。
2.  **计算G和D的损失函数**。
3.  **Backprop** - 计算**G**和**D的**梯度来优化权重。

def train\_step（images，loss\_type ='gan'）：  
 noise = tf.random.normal（\[batch\_size，noise\_dim\]），  
  
 其中tf.GradientTape（）为gen\_tape，tf.GradientTape（）为disc\_tape：  
 generated\_images = dog\_generator（noise，training = True）  
          
 real\_output = dog\_discriminator（images，training = True）  
 fake\_output = dog\_discriminator（generated\_images，training = True）  
          
 gen\_loss = generator\_loss（real\_output，fake\_output，loss\_type，apply\_label\_smoothing = True）  
 disc\_loss = discriminator\_loss（real\_output，fake\_output，loss\_type，  
 apply\_label\_smoothing = True ，label\_noise = True）  
   
 gradients\_of\_generator = gen\_tape.gradient（gen\_loss，dog\_generator.trainable\_variables）  
 gradients\_of\_discriminator = disc\_tape.gradient（disc\_loss，dog\_discriminator.trainable\_variables）  
  
 generator\_optimizer.apply\_gradients（zip（gradients\_of\_generator，dog\_generator.trainable\_variables））  
 discriminator\_optimizer.apply\_gradients（zip（gradients\_of\_discriminator，dog\_discriminator.trainable\_variables））  
      
 return gen\_loss，disc\_loss

让我们定义一些函数来按时代和整体来模拟模型损失。

def plot\_losses（G\_losses，D\_losses，all\_gl，all\_dl，epoch）：  
 plt.figure（figsize =（10,5））  
 plt.title（“Generator and Discriminator Loss  -  EPOCH {}”。format（epoch））  
 plt.plot（ G\_losses，label =“G”）  
 plt.plot（D\_losses，label =“D”）  
 plt.xlabel（“Iterations”）  
 plt.ylabel（“Loss”）  
 plt.legend（）  
 ymax = plt.ylim（）\[1 \]   
 plt.show（）  
      
 plt.figure（figsize =（10,5））  
 plt.plot（np.arange（len（all\_gl）），all\_gl，label ='G'）  
 plt.plot（np.arange（len（ all\_dl）），all\_dl，label ='D'）  
 plt.legend（）  
 ＃plt.ylim（（0，np.min（\[1.1 \* np.max（all\_gl），2 \* ymax\]）））  
 plt.title（ '所有时间损失'）  
 plt.show（）

我们还可以使用以下函数绘制生成图像的网格。

def generate\_and\_save\_images（model，epoch，test\_input，rows，cols）：  
 ＃notice\`training\` 设置为False。  
 ＃这是所有图层都以推理模式运行（batchnorm）。  
 predictions = model（test\_input，training = False）  
 fig = plt.figure（figsize =（14,14））  
 for i in range（predictions.shape \[0\]）：  
 plt.subplot（rows，cols，i + 1）  
 plt .imshow（（predictions \[i，：，：，\] \* 127.5 + 127.5）/ 255.）  
 plt.axis（'off'）  
          
 plt.subplots\_adjust（wspace = 0，hspace = 0）  
 plt.savefig（'image\_at\_epoch\_ { ：04d} .png'.format（epoch））  
 plt.show（）

要生成单个测试图像，我们也可以重用相同的方法。

def generate\_test\_image（model，noise\_dim = noise\_dim）：  
 test\_input = tf.random.normal（\[1，noise\_dim\]）  
 ＃通知\`training\`设置为False。  
 ＃这是所有图层都以推理模式运行（batchnorm）。  
 predictions = model（test\_input，training = False）  
 fig = plt.figure（figsize =（5,5））  
 plt.imshow（（predictions \[0，：，：，\] \* 127.5 + 127.5）/ 255.）  
 plt。 axis（'off'）  
 plt.show（）

评估GAN
=====

我们还没有提到通常如何评估**GAN**。大多数使用基准来评估**GAN**执行情况的研究论文通常基于所谓的**初始分数**。这测量输入图像的两个主要特征：

*   **品种（例如产生不同类型的狗品种）**
*   **区别（或图像的质量）**

如果两件事情都属实，那么得分会很高。如果其中一个或两个都是假的，则分数将很低。得分越高越好。这意味着您的GAN可以生成许多不同的图像。可能的最低分为零。在数学上，尽可能高的分数是无限的，尽管在实践中可能会出现非无限的上限。

在**成立之初分数**是从谷歌的衍生[盗网络](/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)，这对图像进行分类的国家的最先进的深层结构中的一种。通过从我们的GAN通过分类器传递图像，我们可以测量生成的图像的属性。为了产生分数，我们需要计算图像的真实和假分布之间的相似性/距离。这是使用**KL（Kullback-Leibler）分歧公式完成的**：

![](https://miro.medium.com/max/30/1*S6Cp770aAUtMJBK66zLgFw.png?q=20)

![](https://miro.medium.com/max/415/1*S6Cp770aAUtMJBK66zLgFw.png)

这里，**P和Q**是两个测量分布。在这种情况下，**更高的KL发散**意味着更好的结果 - 图像质量相似，并且存在各种各样的标签。在相反的情况下，**低KL分歧**可能是由于低质量或低品种的标签：

![](https://miro.medium.com/max/30/1*NbPx_RCrZv0G7nRIA_cseg.png?q=20)

![](https://miro.medium.com/max/1736/1*NbPx_RCrZv0G7nRIA_cseg.png)

用KL分歧测量性能

FID（Frechet初始距离）
================

**IS的**一个缺点是，如果每个类只生成一个图像，它就会歪曲性能。为了解决这个问题，我们可以使用**FID（Frechet初始距离）**。该度量将两个先前类型的图像定义为具有平均_μ_和协方差Σ（Sigma）的多变量**高斯分布**。让我们看看如何计算这个距离：

![](https://miro.medium.com/max/30/1*c159Dp06UeZUzAj_5JKonw.png?q=20)

![](https://miro.medium.com/max/597/1*c159Dp06UeZUzAj_5JKonw.png)

这里，_x_和_g_表示图像的真实和虚假分布，而_Tr_是结果的对角元素的总和。

> **_较低的FID值意味着更好的图像质量和多样性。_**

![](https://miro.medium.com/max/30/1*YYEKvgxy1WKo6Tn4EE624g.png?q=20)

![](https://miro.medium.com/max/1478/1*YYEKvgxy1WKo6Tn4EE624g.png)

多样性因子如何影响不同数据集的评分的示例

这里还有一些有用的注释，说明为什么**FID**是一个很好的衡量标准：

*   **FID比IS更加稳健。**
*   **如果模型仅为每个类生成一个图像，则距离将很高。因此，FID是图像多样性的更好测量。**
*   **通过计算训练数据集和测试数据集之间的FID，我们应该期望FID为零，因为两者都是真实图像。（虽然通常会有少量错误）**
*   **FID和IS基于特征提取（特征的存在或不存在）。**

MIFID（记忆通知Frechet启动距离）
======================

以下是Kaggle对Generative Dogs竞赛的官方评估工作流程：

![](https://miro.medium.com/max/23/1*EPkyoJAjlDC2QHAKGyHcyg.png?q=20)

![](https://miro.medium.com/max/681/1*EPkyoJAjlDC2QHAKGyHcyg.png)

评估分为两个阶段 - 不同数据集上的公共和私有。MIFID指标使用标准FID和记忆分数计算得出。

我们可以看到，除了**FID**指标外，计算中还增加了一个**记忆分数**。这基本上是**余弦距离**公式，用于测量真实（来自私人数据集的图像）和伪图像之间的相似性。我的猜测是，这样做是为了确保提供给评估内核的图像实际上是由**GAN**生成的，而不仅仅是从真实数据集中复制或修改。

值得庆幸的是，**MIFID**评估员已经由Kaggle团队[（这里）实施](https://www.kaggle.com/wendykan/demo-mifid-metric-for-dog-image-generation-comp)，我们不必担心。

图像压缩和保存功能
=========

我只是**要再**添加两个用于压缩最终的**10K**图像以进行提交并生成临时图像以计算训练期间某些时期之间的**MIFID**。

def zip\_images（filename ='images.zip'）：  
 ＃SAVE TO ZIP FILE NAMED IMAGES.ZIP   
 z = zipfile.PyZipFile（filename，mode ='w'）  
 for k in range（image\_sample\_size）：  
 generated\_image = dog\_generator（tf.random .normal（\[1，noise\_dim\]），training = False）  
 f = str（k）+'。  
 png'img = np.array（generated\_image）  
 img =（img \[0，：，：，：\] + 1.） / 2.   
 img = Image.fromarray（（255 \* img）.astype（'uint8'）。reshape（（image\_height，image\_width，image\_channels）））  
 img.save（f，'PNG'）  
 z.write（f）  
 os .remove（f）  
 #if k％1000 == 0：print（k）  
 z.close（）  
 print（'保存最终图像以供提交。'）   
      
def save\_images（directory = OUT\_DIR）：  
 for k in range（image\_sample\_size）：  
 generated\_image = dog\_generator（tf.random.normal（\[1，noise\_dim\]），training = False）  
 f = str（k）+'。png'f   
 = os.path.join（directory， f）  
 img = np.array（generated\_image）  
 img =（img \[0，：，：，：\] + 1.）/ 2.   
 img = Image.fromarray（（255 \* img）.astype（'uint8'）。reshape （（image\_height，image\_width，image\_channels）））  
 img.save（f，'PNG'）  
 #if k％1000 == 0：print（k）  
 print（'保存的临时图像用于评估。'）

现在终于实施了包含整个过程的最终训练功能。这里还有一些我尚未提及的技术。让我们看看它们是什么。

学习率下降
=====

这个是实验性的并不总是有助于提高性能，但我认为这不会对任何方式造成伤害。这里的想法是为每个训练步骤/步骤降低非常小的**学习率**，以便稳定训练过程并加速收敛（并摆脱局部最小值）。对于这个项目，我使用的是[**余弦学习率衰减**](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay)  的**Tensorflow**，以减少每一次的学习速度`decay_step`迭代。

处理模式崩溃
======

除了**非收敛**和**消失和爆炸梯度之外**，**GAN**有时会遇到另一个称为**模式崩溃的**主要问题。当**G**开始生产有限种类的样品时会发生这种情况。以下是在**MNIST**数据集上训练的**GAN**的**模式折叠的**一个很好的例子，其中**G**连续生成单个类标签的图像：

![](https://miro.medium.com/max/30/1*t5Vr12P6BnrwiQbEbvpUfQ.png?q=20)

G学会通过仅从单个类生成样本来欺骗D，这导致模型失去多样性

我们已经看到一些方法可以消除**模式折叠，**如**标签平滑**，**实例噪声**，权**重初始化**等。我们可以在训练期间应用的另一种方法称为**体验重放**。

**体验重放会**在内存中保留一些最近生成的图像。对于每次`replay_step`迭代，我们在先前的图像上训练**D**以“提醒”前几代网络，从而减少在训练期间**过度拟合**到特定数据批次实例的机会。在这个例子中，我使用的是一种稍微不同的**体验重放**形式，我将为每个训练步骤生成一个新的额外图像以存储在列表中，而不是从之前的迭代中提供实际生成的图像，因为存储数据在**急切执行**期间并非易事。

'''   
 generated\_image = dog\_generator（tf.random.normal（\[1，noise\_dim\]），training = False）  
 exp\_replay.append（generated\_image）  
 if len（exp\_replay）== replay\_step：  
 print（'Executing experience replay ..'）  
 replay\_images = np.array（exp\_replay中p为\[p \[0\]\]）  
 dog\_discriminator（replay\_images，training = True）  
 exp\_replay = \[\]   
'''

_编辑：由于Kaggle在运行约7-8小时后遇到内存问题，我决定不使用经验重播。如果您找到解决方法，请告诉我：D。_

**培训功能**
========

总而言之，培训过程非常简单。还有其他步骤可以显示中间结果，如图像，**丢失**和计算**MIFID**。在学习过程结束时，我们打印出最终评估和最终图像的更大网格。

display\_results = 40   
calculate\_mifid = 100   
replay\_step = 50   
decay\_step = 50   
  
def train（dataset，  
 epochs ）：all\_gl = np.array（\[\]）; all\_dl = np.array（\[\]）  
 用于tqdm（范围（  
          
 epochs ））中的纪元：G\_loss = \[\]; D\_loss = \[\]   
          
 start = time.time（）  
 new\_lr\_d = lr\_initial\_d   
 new\_lr\_g = lr\_initial\_g   
 global\_step = 0   
          
 表示数据集中的  
 image\_batch ：g\_loss，d\_loss = train\_step（image\_batch）  
 global\_step = global\_step + 1   
 G\_loss.append（g\_loss）; D\_loss.append（d\_loss）  
 all\_gl = np.append（all\_gl，np.array（\[G\_loss\]））  
 all\_dl = np.append（all\_dl，np.array（\[D\_loss\]））  
  
 if（epoch + 1）％display\_results == 0或epoch == 0：  
 plot\_losses（G\_loss，D\_loss，all\_gl，all\_dl，epoch + 1）  
 generate\_and\_save\_images（dog\_generator，epoch + 1，seed，rows = 8，cols = 8）  
          
 if （epoch + 1）％calculate\_mifid == 0：   
 OUT\_DIR.mkdir（exist\_ok = True）  
 save\_images（OUT\_DIR）  
 evaluateator = MiFIDEvaluator（MODEL\_PATH，TRAIN\_DIR）  
 fid\_value，distance，mi\_fid\_score = evaluator.evaluate（OUT\_DIR）  
 print（f'FID：{ fid\_value：.5f}'）  
 print（f'distance：{distance：.5f}'）  
 print（f'MiFID：{mi\_fid\_score：.5f}'）  
 shutil.rmtree（OUT\_DIR）  
 print（'删除的临时图像目录。'）  
          
 ＃余弦学习率衰减  
 if（epoch + 1）％decay\_step == 0：  
 new\_lr\_d = tf.train.cosine\_decay（new\_lr\_d，min（global\_step，lr\_decay\_steps），lr\_decay\_steps）  
 new\_lr\_g = tf。 train.cosine\_decay（new\_lr\_g，min（global\_step，lr\_decay\_steps），lr\_decay\_steps）  
 generator\_optimizer = tf.train.AdamOptimizer（learning\_rate = new\_lr\_d，beta1 = 0.5）  
 discriminator\_optimizer = tf.train.AdamOptimizer（learning\_rate = new\_lr\_g，beta1 = 0.5）   
  
 print（' Epoch：{}为{} sec'.format计算（epoch + 1，time.time（） -  start））  
 print（'Gen\_loss mean：'，np.mean（G\_loss），'std：'，np.std（ G\_loss））  
 print（'disc\_loss mean：'，np.mean（D\_loss），'std：'，np.std（D\_loss））  
  
 ＃在最后一个纪元后生成并重复进程  
 generate\_and\_save\_images（dog\_generator，epochs，seed，rows = 8，cols = 8）  
 checkpoint.save（file\_prefix = checkpoint\_prefix）  
     
 OUT\_DIR.mkdir（exist\_ok = True）  
 save\_images（OUT\_DIR）  
 evaluationator = MiFIDEvaluator（MODEL\_PATH，TRAIN\_DIR）  
 fid\_value，distance，mi\_fid\_score = evaluator.evaluate（OUT\_DIR）  
 print（f'FID：{ fid\_value：.5f}'）  
 print（f'distance：{distance：.5f}'）  
 print（f'MiFID：{mi\_fid\_score：.5f}'）  
 shutil.rmtree（OUT\_DIR）  
 print（'Removed temporary image directory'。 ）  
      
    
 print（'Final epoch。'）

以下是训练期间生成的一些狗图像：

![](https://miro.medium.com/max/30/1*ByRDA7N5DSEBEAyNGtGlGQ.png?q=20)

Epoch 120 - MIFID~90.5

![](https://miro.medium.com/max/30/1*u0XOmQ8_5nSBItT7f7HI7A.png?q=20)

Epoch 200 - MIFID~64.8

![](https://miro.medium.com/max/30/1*SkG-eUZOBIiW0U6kWDdY3A.png?q=20)

Epoch 280（最终） - MIFID~60.99

正如我们所观察到的，MIFID在280个时期（约8小时）内稳步提升。我在比赛中使用这个模型获得的最高分是**55.87**。学习过程确实有点随机，所以我认为**\[50,65\]**区域的分数应该是现实的。如果你有时间，可以随意训练这个模型，因为它有可能继续改进:)。

最后，我将向您展示如何制作一个有趣的**GIF，**以便看到**DCGAN**学习过程的一个很好的小模拟（来自Tensorflow的[DCGAN教程的](https://www.tensorflow.org/beta/tutorials/generative/dcgan)代码）。

anim\_file ='dcgan.gif'，  
  
其中imageio.get\_writer（anim\_file，mode ='I'）作为writer：  
 filenames = glob.glob（'image \* .png'）  
 filenames = sorted（filenames）  
 last = -1   
 for i，filename in enumerate（filenames）：  
 frame = 1 \*（i \*\* 2）  
 if round（frame）> round（last）：  
 last = frame   
 else：  
 continue   
 image = imageio.imread（filename）  
 writer.append\_data（image）  
 image = imageio .imread（filename）  
 writer.append\_data（image）如果IPython.version\_info>（6,2,0，''）  
  
导入IPython   
：  
 IPython.display.Image（filename = anim\_file）

结论
==

总而言之，**DCGAN**似乎对超参数选择极其敏感，并且在训练期间可能会出现很多问题，包括**模式崩溃**。它们的计算密集程度非常高，为运行时间**约为9小时**构建高分模型非常困难。幸运的是，有一整套可能的方法和技术，这些方法和技术都有详细记录，可以很容易地应用到您的模型中，以稳定培训过程。

就我个人而言，玩这些技术并在此过程中打破一些内核真的很有趣：D。请随意在评论中留下任何建议（用于改进模型或修复我搞砸了的东西）。

非常感谢**Chris Deotte，Nanashi，Chad Malla和Nirjhar Roy**在比赛期间的Kaggle内核和示例。我会在下面留下链接给他们。

总的来说，这是我参加过的第一次**Kaggle**比赛，这是一个非常有趣的方法，可以学习**GAN**并玩弄它们。这似乎是Kaggle的第一次涉及生成建模的竞赛，让我们希望将来会出现更多令人兴奋的挑战;）。

参考
==

有用的内核和笔记本
=========

\[1\]。我之前关于[EDA和图像预处理的](https://www.kaggle.com/jadeblue/dog-generator-starter-eda-preprocessing)内核[](https://www.kaggle.com/jadeblue/dog-generator-starter-eda-preprocessing)

\[2\]。[Xml解析和裁剪到指定的边界框](https://www.kaggle.com/paulorzp/show-annotations-and-breeds)

\[3\]。[具有插值的图像裁剪方法](https://www.kaggle.com/amanooo/wgan-gp-keras)

\[4\]。[Chad Malla的另一个伟大的基于Keras的DCGAN方法](https://www.kaggle.com/cmalla94/dcgan-generating-dog-images-with-tensorflow)

\[5\]。[DCGAN无法提升您的模型性能](https://www.kaggle.com/c/generative-dog-images/discussion/98595)

\[6\]。[Tensorflow DCGAN教程](https://www.tensorflow.org/beta/tutorials/generative/dcgan)

\[7\]。[DCGAN狗图像由Nanashi](https://www.kaggle.com/jesucristo/introducing-dcgan-dogs-images)

\[8\]。[GAN犬首发24-Jul - Nirjhar Roy的Custom Layers](https://www.kaggle.com/phoenix9032/gan-dogs-starter-24-jul-custom-layers)

\[9\]。[由Chris Deotte监督的Generative Dog Net](https://www.kaggle.com/cdeotte/supervised-generative-dog-net)

\[10\]。[我最好的参赛作品](https://www.kaggle.com/jadeblue/dogdcgan-v6-ksize)

研究论文，帖子和讨论
==========

\[1\]。Ian Goodfellow，J。Pouget-Abadie，M。Mirza，B。Xu **，** S。Ozair，Y。Bengio **，**[Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661.pdf)（2014）

\[2\]。J. Rocca，[了解生成性对抗网络（GAN）](/understanding-generative-adversarial-networks-gans-cd6e4651a29)

\[3\]。J. Brownlee，[生成对抗网络（GAN）的温和介绍](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)

\[4\]。A. Radford，L。Metz，S。Chintala，[使用深度卷积生成对抗网络的无监督表示学习](https://arxiv.org/abs/1511.06434)（2015）

\[5\]。J. Hui，[GAN - DCGAN（深度卷积生成对抗网络）](https://medium.com/@jonathan_hui/gan-dcgan-deep-convolutional-generative-adversarial-networks-df855c438f)

\[6\]。S.Yadav，[神经网络中的权重初始化技术](/weight-initialization-techniques-in-neural-networks-26c649eb3b78)

\[7\]。T. Miyato，T。Kataoka，M。Koyama，Y。Yoshida，[Generative Adversarial Networks的频谱归一化](https://arxiv.org/pdf/1802.05957.pdf)（2018）

\[8\]。IShengFang，[光谱归一化在](https://github.com/IShengFang/SpectralNormalizationKeras) Keras [实施](https://github.com/IShengFang/SpectralNormalizationKeras)

\[9\]。C. Cosgrove，[光谱归一化解释](https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html)

\[10\]。J. Hui，[GAN - 改善GAN表现的方法](/gan-ways-to-improve-gan-performance-acf37f9f59b)

\[11\]。J. Brownlee，[如何在Keras实施GAN黑客训练稳定模型](https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/)

\[12\]。Y. Jiao，[GANS的伎俩](https://lanpartis.github.io/deep%20learning/2018/03/12/tricks-of-gans.html)

\[13\]。CKSønderby，[Instance Noise：稳定GAN训练的技巧](https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/)

\[14\]。J. Hui，[GAN - RSGAN＆RaGAN（新一代成本函数。）](https://medium.com/@jonathan_hui/gan-rsgan-ragan-a-new-generation-of-cost-function-84c5374d3c6e)

\[15\]。D. Mack，[对初始分数的简单解释](https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a)

\[16\]。J. Hui，[GAN - 如何衡量GAN绩效？](https://medium.com/@jonathan_hui/gan-how-to-measure-gan-performance-64b988c47732)

\[17\]。[你需要的只是GAN Hacks](https://www.kaggle.com/c/generative-dog-images/discussion/98595#latest-582912)

\[18\]。[如何训练你敏感的GAN - 似乎有用的东西。](https://www.kaggle.com/c/generative-dog-images/discussion/102155#latest-599429)

\[19\]。[解释度量标准FID](https://www.kaggle.com/c/generative-dog-images/discussion/97809#latest-591866)