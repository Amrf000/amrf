![自动编码器高级指南](https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924)
自动编码器高级指南
=========

从最基本到最花哨的自动编码器工具箱。
------------------

[![Shreya Chaudhary](https://miro.medium.com/fit/c/48/48/0*vo-8HYNssBKYpFcJ.jpg)](/@schaudhary_11674?source=post_page-----b103ccd45924----------------------)

[Shreya Chaudhary](/@schaudhary_11674?source=post_page-----b103ccd45924----------------------)

跟随

[8月22日](/a-high-level-guide-to-autoencoders-b103ccd45924?source=post_page-----b103ccd45924----------------------) · 6 分钟阅读

在机器学习和人工智能的奇妙世界中，存在称为自动编码器的这种结构。自动编码器是一种类型的神经网络，它是无监督学习（或某些半无监督学习）的一部分。有许多不同类型的自动编码器用于许多目的，一些生成，一些预测等。本文应为您提供工具箱和不同类型的自动编码器指南。

传统自动编码器（AE）
===========

![](https://miro.medium.com/max/30/1*ldhBBbw6iFbEc5EkLAgKYg.png?q=20)

![](https://miro.medium.com/max/1764/1*ldhBBbw6iFbEc5EkLAgKYg.png)

基本的自动编码器

自动编码器的基本类型如上所示。它由输入层（第一层），隐藏层（黄色层）和输出层（最后一层）组成。网络的目标是使输出层与输入层完全相同。隐藏层用于特征提取或识别决定结果的特征。从第一层到隐藏层的过程称为编码。从隐藏层到输出层的过程称为解码。编码和解码过程使自动编码器变得特别。黄色层有时被称为瓶颈隐藏层。

从这里开始，有许多不同类型的自动编码器。

问题：隐藏层过度完整
==========

我们要看的前几个是解决过度完整的隐藏层问题。

![](https://miro.medium.com/max/30/1*tLmp5ozxunmdPseuyX_WuQ.png?q=20)

![](https://miro.medium.com/max/1808/1*tLmp5ozxunmdPseuyX_WuQ.png)

过度完整的隐藏层

需要注意的重要部分是隐藏层多于输入/输出层。如果他们的数据中具有比平常更多的功能，则需要这样做。这样做的主要问题是输入可以没有任何变化; 没有任何真正的功能_提取_。看看下面的例子：

![](https://miro.medium.com/max/30/1*kNTLYQm9FlXslYVUo9QrEw.png?q=20)

![](https://miro.medium.com/max/1724/1*kNTLYQm9FlXslYVUo9QrEw.png)

不使用灰色节点; 蓝色节点根本没有改变。

输入层和输出层之间没有真正的变化; 他们只是保持不变。此外，根本没有使用两个隐藏层节点。为避免这种情况，至少有三种方法：

解答A：稀疏自动编码器
-----------

![](https://miro.medium.com/max/30/1*YoseSRJ0HssJor64ypBb6w.png?q=20)

![](https://miro.medium.com/max/1686/1*YoseSRJ0HssJor64ypBb6w.png)

稀疏自动编码器

简而言之，稀疏自动编码器能够“敲除”隐藏层中的一些神经元，迫使自动编码器使用其所有神经元。它不再能够通过某些节点记住输入，因为在每次运行中，这些节点可能不是活动的节点。

解决方案B：去噪自动编码器（DAE）
------------------

![](https://miro.medium.com/max/30/1*BN7Gr1zby87mIMc7lzEoSA.png?q=20)

![](https://miro.medium.com/max/1728/1*BN7Gr1zby87mIMc7lzEoSA.png)

一些输入变为0。

我们改变了稀疏自动编码器中的隐藏层。另一种选择是改变输入。在去噪自动编码器中，一些输入变为零（随机）。一旦输入，输出将与原始（非零）输入进行比较。这与稀疏自动编码器具有相似的目的，但是，这次，归零的自动编码器位于不同的位置。

解决方案C：压缩自动编码器（CAE）
------------------

![](https://miro.medium.com/max/30/1*dT-BbzhcdJvHJ2U64xqXEA.png?q=20)

![](https://miro.medium.com/max/1702/1*dT-BbzhcdJvHJ2U64xqXEA.png)

黑暗的圆圈代表略微修改的输出。

我们更改了输入图层，隐藏图层，现在我们将更改输出图层。自动编码器通过一种称为反向传播的方法进行训练; 在进行此算法时，在收缩自动编码器中，输出略有改变，但并未完全归零（与过去的算法一样）。在反向传播时对输出层的这种改变是阻止纯记忆的原因。

为什么解决这个问题？
----------

这些网络的一些实际应用包括标记图像数据以进行分割，去噪图像（这是DAE的明显选择），检测异常值以及填充图像中的间隙。其中许多应用程序还可以与SAE一起使用，下面将对其进行说明。

* * *

够了这个问题。现在我们将提到更强大的自动编码器的其他变体。

堆叠式自动编码器（SAE）
=============

![](https://miro.medium.com/max/30/0*ImIMckxBvxVSCTlc.png?q=20)

![](https://miro.medium.com/max/827/0*ImIMckxBvxVSCTlc.png)

资料来源：[https](https://www.researchgate.net/figure/Stacked-autoencoders-architecture_fig21_319524552)：[//www.researchgate.net/figure/Stacked-autoencoders-architecture\_fig21\_319524552](https://www.researchgate.net/figure/Stacked-autoencoders-architecture_fig21_319524552)

堆叠式自动编码器开始看起来很像神经网络。本质上，SAE是许多自动编码器，它们与多层编码和解码结合在一起。这允许算法具有更多层，更多权重，并且最有可能最终更健壮。

不仅要了解SAE而且要了解其他形式的AE的方法是，层也可以是卷积层和反卷积层; 这对图像处理更方便。

SAE和AE的一些用途通常包括分类和图像大小调整。这些是自动编码器的特征提取工具的两种实际用途; 任何其他用途的特征提取对自动编码器都很有用。

变分自动编码器（VAE）
============

![](https://miro.medium.com/max/30/0*rTeNcw5CDu_IsvC7.jpg?q=20)

![](https://miro.medium.com/max/1500/0*rTeNcw5CDu_IsvC7.jpg)

资料来源：[http](http://kvfrans.com/variational-autoencoders-explained/)：[//kvfrans.com/variational-autoencoders-explained/](http://kvfrans.com/variational-autoencoders-explained/)

这可能是自动编码器最常用的变体：生成编码器。它也比其他人复杂得多。

简而言之，VAE类似于SAE，但它们能够分离解码器。在中心，有两个向量，然后组合成一个潜在的向量。这是什么意思？好吧，如果理论上从理论上把刚刚出现瓶颈的隐藏层从SAE中取出并要求它生成给定随机向量的图像，那么它很可能产生噪声。有很多随机性，只有某些区域是提供真实图像的向量。

![](https://miro.medium.com/max/30/1*vrMbPTdno3KMTN5MJK1yJg.png?q=20)

![](https://miro.medium.com/max/1240/1*vrMbPTdno3KMTN5MJK1yJg.png)

黄色部门是很好的载体。所有的粉红色区域都会产生噪音。

由于获得图像生成矢量的机会很小，平均值和标准偏差有助于将这些黄色区域挤压成一个称为潜在空间的区域。反过来，这将从中采样以产生最终图像。潜在空间内的一切都应该产生一个图像。

从这里开始，可以取出编码部分，结果应该是生成器。

解缠结的变分自动编码器
===========

这是VAE的径流，略有变化。这基本上允许每个矢量控制图像的一个（且仅一个）特征。

看看下面的例子：

![](https://miro.medium.com/max/30/1*IVQUTGedYmmmjr0H0HKkuQ.png?q=20)

![](https://miro.medium.com/max/1534/1*IVQUTGedYmmmjr0H0HKkuQ.png)

左边的两个使用解缠结的VAE，左边的一个是正常的VAE。（来源：[https](https://arxiv.org/pdf/1707.08475.pdf)：[//arxiv.org/pdf/1707.08475.pdf](https://arxiv.org/pdf/1707.08475.pdf)）

请注意，在解开的选项中，只有一个功能被更改（例如左转，右转，距离等）。然而，在纠缠中，似乎有许多特征同时发生变化。单个更改改变单个特征的能力是解开的VAE的重点。

这样做的方法是在原始VAE中添加另一个参数，考虑输入向量中每次变化后模型的变化程度。从那里，权重将相应调整。

* * *

而且......现在就是这样。我希望你喜欢这个工具箱。我使用了很多文章和视频，所有这些都是非常好的读/手表。如果有任何方法可以改进，或者如果您有任何意见或建议，我很乐意听取您的反馈意见。谢谢！

使用的来源
-----

_注意：除非另有说明，否则所有图像均由我自己设计。_

[

神经网络 - 深度自动编码器与堆叠式自动编码器的区别


------------------------------

### 

该领域的术语不固定，切割明确，定义明确，不同的研究可能意味着不同......

#### 

stackoverflow.com



](https://stackoverflow.com/questions/49296951/neural-networks-difference-between-deep-autoencoder-and-stacked-autoencoder?source=post_page-----b103ccd45924----------------------#targetText=As%20I%20understand%20it%2C%20the,greedy%2C%20layer%2Dwise%20approach.)

[

基于堆积自动编码器的深度神经网络实现齿轮箱故障诊断


-----------------------------

### 

机械故障诊断在现代制造业中至关重要，因为早期检测可以避免一些......

#### 

www.hindawi.com



](https://www.hindawi.com/journals/mpe/2018/5105709/?source=post_page-----b103ccd45924----------------------)

[

教程 - 什么是变分自动编码器？ - Jaan Altosaar


------------------------------------

### 

为什么深度学习研究人员和概率机器学习人员在讨论变分时会感到困惑......

#### 

jaan.io









](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/?source=post_page-----b103ccd45924----------------------)

[

变分自动编码器解释


-------------

### 

在我之前关于生成对抗性网络的帖子中，我采用了一种简单的方法来训练可以...

#### 

kvfrans.com



](http://kvfrans.com/variational-autoencoders-explained/?source=post_page-----b103ccd45924----------------------)

[https://www.youtube.com/watch?v=9zKuYvjFFS8](https://www.youtube.com/watch?v=9zKuYvjFFS8)

[https://www.youtube.com/watch?v=fcvYpzHmhvA](https://www.youtube.com/watch?v=fcvYpzHmhvA)

[http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)

[https://arxiv.org/pdf/1707.08475.pdf](https://arxiv.org/pdf/1707.08475.pdf)